apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-bsc
  labels:
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.10
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: v1.1.8
    app.kubernetes.io/managed-by: Helm
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-config
  labels:
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.10
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: v1.1.8
    app.kubernetes.io/managed-by: Helm
data:
  config.toml: |-
    [Eth]
    NetworkId = 56
    NoPruning = false
    NoPrefetch = false
    LightPeers = 100
    UltraLightFraction = 75
    TrieTimeout = 100000000000
    EnablePreimageRecording = false
    EWASMInterpreter = ""
    EVMInterpreter = ""
    DisablePeerTxBroadcast = true

    [Eth.Miner]
    GasFloor = 30000000
    GasCeil = 40000000
    GasPrice = 1000000000
    Recommit = 10000000000
    Noverify = false

    [Eth.TxPool]
    Locals = []
    NoLocals = true
    Journal = "transactions.rlp"
    Rejournal = 3600000000000
    PriceLimit = 1000000000
    PriceBump = 10
    AccountSlots = 512
    GlobalSlots = 10000
    AccountQueue = 256
    GlobalQueue = 5000
    Lifetime = 10800000000000

    [Eth.GPO]
    Blocks = 20
    Percentile = 60
    OracleThreshold = 20

    [Node]
    IPCPath = "geth.ipc"
    HTTPHost = "0.0.0.0"
    NoUSB = true
    InsecureUnlockAllowed = false
    HTTPPort = 8575
    HTTPVirtualHosts = ["*"]
    HTTPModules = ["eth", "net", "web3", "txpool", "parlia"]
    WSPort = 8576
    WSOrigins = ["*"]
    WSModules = ["net", "web3", "eth"]

    [Node.HTTPTimeouts]
    ReadTimeout = 30000000000
    WriteTimeout = 30000000000
    IdleTimeout = 120000000000

    [Node.LogConfig]
    FilePath = "bsc.log"
    MaxBytesSize = 10485760
    Level = "info"
    FileRoot = ""

    # keep this section the last one, as we may append trusted nodes via config generation
    [Node.P2P]
    EnableMsgEvents = false
    MaxPeers = 50
    NoDiscovery = false
    ListenAddr = ":30311"
    BootstrapNodes = ["enode://1cc4534b14cfe351ab740a1418ab944a234ca2f702915eadb7e558a02010cb7c5a8c295a3b56bcefa7701c07752acd5539cb13df2aab8ae2d98934d712611443@52.71.43.172:30311","enode://28b1d16562dac280dacaaf45d54516b85bc6c994252a9825c5cc4e080d3e53446d05f63ba495ea7d44d6c316b54cd92b245c5c328c37da24605c4a93a0d099c4@34.246.65.14:30311","enode://5a7b996048d1b0a07683a949662c87c09b55247ce774aeee10bb886892e586e3c604564393292e38ef43c023ee9981e1f8b335766ec4f0f256e57f8640b079d5@35.73.137.11:30311"]
    StaticNodes = ["enode://f3cfd69f2808ef64838abd8786342c0b22fdd28268703c8d6812e26e109f9a7cb2b37bd49724ebb46c233289f22da82991c87345eb9a2dadeddb8f37eeb259ac@18.180.28.21:30311","enode://ae74385270d4afeb953561603fcedc4a0e755a241ffdea31c3f751dc8be5bf29c03bf46e3051d1c8d997c45479a92632020c9a84b96dcb63b2259ec09b4fde38@54.178.30.104:30311","enode://d1cabe083d5fc1da9b510889188f06dab891935294e4569df759fc2c4d684b3b4982051b84a9a078512202ad947f9240adc5b6abea5320fb9a736d2f6751c52e@54.238.28.14:30311","enode://f420209bac5324326c116d38d83edfa2256c4101a27cd3e7f9b8287dc8526900f4137e915df6806986b28bc79b1e66679b544a1c515a95ede86f4d809bd65dab@54.178.62.117:30311","enode://c0e8d1abd27c3c13ca879e16f34c12ffee936a7e5d7b7fb6f1af5cc75c6fad704e5667c7bbf7826fcb200d22b9bf86395271b0f76c21e63ad9a388ed548d4c90@54.65.247.12:30311","enode://f1b49b1cf536e36f9a56730f7a0ece899e5efb344eec2fdca3a335465bc4f619b98121f4a5032a1218fa8b69a5488d1ec48afe2abda073280beec296b104db31@13.114.199.41:30311","enode://4924583cfb262b6e333969c86eab8da009b3f7d165cc9ad326914f576c575741e71dc6e64a830e833c25e8c45b906364e58e70cdf043651fd583082ea7db5e3b@18.180.17.171:30311","enode://4d041250eb4f05ab55af184a01aed1a71d241a94a03a5b86f4e32659e1ab1e144be919890682d4afb5e7afd837146ce584d61a38837553d95a7de1f28ea4513a@54.178.99.222:30311","enode://b5772a14fdaeebf4c1924e73c923bdf11c35240a6da7b9e5ec0e6cbb95e78327690b90e8ab0ea5270debc8834454b98eca34cc2a19817f5972498648a6959a3a@54.170.158.102:30311","enode://f329176b187cec87b327f82e78b6ece3102a0f7c89b92a5312e1674062c6e89f785f55fb1b167e369d71c66b0548994c6035c6d85849eccb434d4d9e0c489cdd@34.253.94.130:30311","enode://cbfd1219940d4e312ad94108e7fa3bc34c4c22081d6f334a2e7b36bb28928b56879924cf0353ad85fa5b2f3d5033bbe8ad5371feae9c2088214184be301ed658@54.75.11.3:30311","enode://c64b0a0c619c03c220ea0d7cac754931f967665f9e148b92d2e46761ad9180f5eb5aaef48dfc230d8db8f8c16d2265a3d5407b06bedcd5f0f5a22c2f51c2e69f@54.216.208.163:30311","enode://352a361a9240d4d23bb6fab19cc6dc5a5fc6921abf19de65afe13f1802780aecd67c8c09d8c89043ff86947f171d98ab06906ef616d58e718067e02abea0dda9@79.125.105.65:30311","enode://bb683ef5d03db7d945d6f84b88e5b98920b70aecc22abed8c00d6db621f784e4280e5813d12694c7a091543064456ad9789980766f3f1feb38906cf7255c33d6@54.195.127.237:30311","enode://11dc6fea50630b68a9289055d6b0fb0e22fb5048a3f4e4efd741a7ab09dd79e78d383efc052089e516f0a0f3eacdd5d3ffbe5279b36ecc42ad7cd1f2767fdbdb@46.137.182.25:30311","enode://21530e423b42aed17d7eef67882ebb23357db4f8b10c94d4c71191f52955d97dc13eec03cfeff0fe3a1c89c955e81a6970c09689d21ecbec2142b26b7e759c45@54.216.119.18:30311","enode://d61a31410c365e7fcd50e24d56a77d2d9741d4a57b295cc5070189ad90d0ec749d113b4b0432c6d795eb36597efce88d12ca45e645ec51b3a2144e1c1c41b66a@34.204.129.242:30311","enode://bb91215b1d77c892897048dd58f709f02aacb5355aa8f50f00b67c879c3dffd7eef5b5a152ac46cdfb255295bec4d06701a8032456703c6b604a4686d388ea8f@75.101.197.198:30311","enode://786acbdf5a3cf91b99047a0fd8305e11e54d96ea3a72b1527050d3d6f8c9fc0278ff9ef56f3e56b3b70a283d97c309065506ea2fc3eb9b62477fd014a3ec1a96@107.23.90.162:30311","enode://4653bc7c235c3480968e5e81d91123bc67626f35c207ae4acab89347db675a627784c5982431300c02f547a7d33558718f7795e848d547a327abb111eac73636@54.144.170.236:30311","enode://c6ffd994c4ef130f90f8ee2fc08c1b0f02a6e9b12152092bf5a03dd7af9fd33597d4b2e2000a271cc0648d5e55242aeadd6d5061bb2e596372655ba0722cc704@54.147.151.108:30311","enode://99b07e9dc5f204263b87243146743399b2bd60c98f68d1239a3461d09087e6c417e40f1106fa606ccf54159feabdddb4e7f367559b349a6511e66e525de4906e@54.81.225.170:30311","enode://1479af5ea7bda822e8747d0b967309bced22cad5083b93bc6f4e1d7da7be067cd8495dc4c5a71579f2da8d9068f0c43ad6933d2b335a545b4ae49a846122b261@52.7.247.132:30311","enode://43562d35f274d9e93f5ccac484c7cb185eabc746dbc9f3a56c36dc5a9ef05a3282695de7694a71c0bf4600651f49395b2ee7a6aaef857db2ac896e0fcbe6b518@35.73.15.198:30311","enode://08867e57849456fc9b0b00771f53e87ca6f2dd618c23b34a35d0c851cd484a4b7137905c5b357795025b368e4f8fe4c841b752b0c28cc2dbbf41a03d048e0e24@35.74.39.234:30311"]
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-env
  labels:
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.10
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: v1.1.8
    app.kubernetes.io/managed-by: Helm
data:
  SNAPSHOT_URL_123: rsync://bsc-bootnode.bsc:1873/bsc
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-probe-env
  labels:
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.10
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: v1.1.8
    app.kubernetes.io/managed-by: Helm
data:
  env.txt: |-
    ReadinessProbeTimestampDistinct=300
    LivenessProbeTimestampDistinct=300
    StartupProbeTimestampDistinct=300
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-scripts
  labels:
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.10
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: v1.1.8
    app.kubernetes.io/managed-by: Helm
data:
  check_node_health.sh: |-
    #!/usr/bin/env sh
    set -ex # -e exits on error

    usage() { echo "Usage: $0 <rpc_endpoint> <max_lag_in_seconds> <last_synced_block_file>]" 1>&2; exit 1; }

    rpc_endpoint="$1"
    max_lag_in_seconds="$2"
    last_synced_block_file="$3"

    if [ -z "${rpc_endpoint}" ] || [ -z "${max_lag_in_seconds}" ] || [ -z "${last_synced_block_file}" ]; then
        usage
    fi

    block_number=$(geth --datadir=/data attach --exec "eth.blockNumber")

    if [ -z "${block_number}" ] || [ "${block_number}" == "null" ]; then
        echo "Block number returned by the node is empty or null"
        exit 1
    fi

    if [ ! -f ${last_synced_block_file} ]; then
        old_block_number="";
    else
        old_block_number=$(cat ${last_synced_block_file});
    fi;

    if [ "${block_number}" != "${old_block_number}" ]; then
      mkdir -p $(dirname "${last_synced_block_file}")
      echo ${block_number} > ${last_synced_block_file}
    fi

    file_age=$(($(date +%s) - $(date -r ${last_synced_block_file} +%s)));
    max_age=${max_lag_in_seconds};
    echo "${last_synced_block_file} age is $file_age seconds. Max healthy age is $max_age seconds";
    if [ ${file_age} -lt ${max_age} ]; then exit 0; else exit 1; fi
  check_node_readiness.sh: |-
    #!/usr/bin/env sh
    set -ex

    public_bsc_node=$3
    allowed_number_of_distinct_between_blocks=$2
    allowed_number_of_time_gap_between_blocks=$2
    local_node_endpoint=${4:-}

    # Retrieving latest block timestamp from public bsc node
    function get_public_block {
        geth --datadir=/tmp attach $public_bsc_node --exec "eth.blockNumber" || exit 0
    }

    # Retrieving latest local block number
    function get_local_block {
        geth --config=/config/config.toml --datadir=/data attach $local_node_endpoint --exec "eth.blockNumber"
    }

    # Retrieving latest block timestamp of a local bsc node
    function get_local_timestamp {
        geth --config=/config/config.toml --datadir=/data attach $local_node_endpoint --exec "eth.getBlock(eth.blockNumber).timestamp"
    }

    case "$1" in
            # If public latest block number differs with local latest block for more than 10 blocks => fail, otherwise okay.
            --distinct-blocks)
                if [[ $(expr  $(get_public_block) - $(get_local_block)) -le $allowed_number_of_distinct_between_blocks ]]
                then
                  echo "Current block gap is lower that $allowed_number_of_distinct_between_blocks"
                  exit 0
                else
                  echo "Current block gap is higher that $allowed_number_of_distinct_between_blocks."
                  exit 1
                fi
                ;;
            # If local latest block's timestamp lower than current timestamp for more than 600 seconds (10 minutes)
            --timestamp-distinct)
                if [[ $(expr $(date +%s) - $(get_local_timestamp)) -le $allowed_number_of_time_gap_between_blocks ]]
                then
                  echo  "Current timestamp gap is lower that $allowed_number_of_time_gap_between_blocks"
                  exit 0
                else
                  echo  "Current timestamp gap is higher that $allowed_number_of_time_gap_between_blocks"
                  exit 1
                fi
                ;;
            *)
                echo "Usage: $0 {--distinct-blocks|--timestamp-distinct} {blocks-distinct,|time-range-distinct-seconds} {public-bsc-node-endpoint}
                      Blocks check:
                              $0 --distinct-blocks 10 https://bsc-dataseed1.binance.org
                      Timestamp check:
                              $0 --timestamp-distinct 300"
                exit 1
    esac
  get_nodekey.py: |-
    #!/usr/bin/env python
    import os
    import re
    import sys

    KEYS = ['']

    def get_nodekey():
        node_id = re.search(r'^.*-(\d+)$', os.uname()[1]).group(1)
        return KEYS[int(node_id)]

    if __name__ == '__main__':
        nodekey = get_nodekey()
        with open("/data/geth/nodekey", "w") as f:
            sys.stdout.write("Node key: {}".format(nodekey))
            f.write(nodekey)
  get_nodekey_ip.py: |-
    #!/usr/bin/env python
    import os
    import sys
    import ipaddress
    import json

    nodeKeysFileName = "/generated-config/nodekeys"

    def get_nodekey(nodeKeysFileName):
        addr=ipaddress.ip_address(os.environ['MY_POD_IP'])
        net=ipaddress.ip_network("192.168.0.0/20")
        if addr in net:
          node_id=int(addr)-int(net[0])
        else:
          sys.stdout.write("Pod address "+str(addr)+" in not inside network "+str(net))
          sys.exit(1)
        with open(nodeKeysFileName, "r") as f:
          KEYS=json.load(f)
          return KEYS[node_id]

    if __name__ == '__main__':
        nodekey = get_nodekey(nodeKeysFileName)
        with open("/data/geth/nodekey", "w") as f:
            sys.stdout.write("Node key: {}".format(nodekey))
            f.write(nodekey)
  generate_node_config.sh: |-
    #!/usr/bin/env sh

    set -ex # -e exits on error
    SRC_DIR=/config
    DST_DIR=/generated-config
    CONFIG_NAME=config.toml
    TRUSTED_NODES_SRC_URL=gs://bucket/trusted_nodes
    NODEKEYS_SRC_URL=gs://bucket/nodekeys
    NODEKEYS=nodekeys
    TRUSTED_NODES=trusted_nodes



    # check if we really need to generate config
    if [ "${GENERATE_CONFIG}" != "true" ];then
      echo "Config generation disabled, copying instead"
      cp -f "${SRC_DIR}/${CONFIG_NAME}" "${DST_DIR}/${CONFIG_NAME}"
      exit 0
    fi

    # config generation
    cd /tmp

    gsutil cp "${TRUSTED_NODES_SRC_URL}" "${TRUSTED_NODES}"

    # # https://askubuntu.com/a/1175271
    # # replace a matching line with a file content
    # sed  -e "/^TrustedNodes.*/{r${TRUSTED_NODES}" -e "d}" "${SRC_DIR}/${CONFIG_NAME}" > "${DST_DIR}/${CONFIG_NAME}"
    #
    # if [ -s "${DST_DIR}/${CONFIG_NAME}" ];then
    #   echo "Resulting config is empty"
    #   exit 1
    # fi

    cp "${SRC_DIR}/${CONFIG_NAME}" "${DST_DIR}/${CONFIG_NAME}"
    echo >> "${DST_DIR}/${CONFIG_NAME}"
    cat "${TRUSTED_NODES}" >> "${DST_DIR}/${CONFIG_NAME}"
  init_from_snaphot.sh: |-
    #!/usr/bin/env sh
    set -ex # -e exits on error

    DATA_DIR="/data"
    TEST_FILE="${DATA_DIR}/.initialized"
    SNAPSHOT_URL=""

    if [ -f ${TEST_FILE} ]; then
        echo "Blockchain already initialized. Exiting..."
        exit 0
    fi

    # Cleanup
    rm -rf ${DATA_DIR}/geth

    # Download & extract snapshot
    # special handling of zstd
    if [[ "${SNAPSHOT_URL}" =~ "\.zst$" ]]; then
      wget ${SNAPSHOT_URL} -O - | mbuffer -m5% -q -l /tmp/m1.log | zstd -d -T0 | mbuffer -m5% -q -l /tmp/m2.log | tar -b 2048 --overwrite -x -C ${DATA_DIR}
    else
      wget ${SNAPSHOT_URL} -O - | tar --overwrite -x -C ${DATA_DIR}
    fi


    # Mark data dir as initialized
    touch ${TEST_FILE}
  init_from_rsync.sh: |-
    #!/usr/bin/env sh
    set -ex # -e exits on error

    DATA_DIR="/data"
    TEST_FILE="${DATA_DIR}/.initialized"
    SNAPSHOT_URL="rsync://192.168.8.4/snapshot/geth/node/geth"

    # get statefulset pod number from pre-defined env var
    STS_POD_NUMBER=$(echo $MY_POD_NAME|sed -r 's/^.+\-([0-9]+)$/\1/')
    #generate variable name to check for URL override, it should be like SNAPSHOT_URL_123, if present
    VAR_NAME=\$SNAPSHOT_URL_${STS_POD_NUMBER}
    #get the URL, if any
    NEW_URL=$(eval echo $VAR_NAME)
    if [ ! -z "$NEW_URL" ];then
      SNAPSHOT_URL=$NEW_URL
    fi

    if [ -f ${TEST_FILE} ]; then
        echo "Blockchain already initialized. Exiting..."
        exit 0
    fi

    set +e
    # remove missing files to cleanup
    rsync -av --inplace --delete-before ${SNAPSHOT_URL}/ ${DATA_DIR}/
    # add more times to catch up
    rsync -av --inplace ${SNAPSHOT_URL}/ ${DATA_DIR}/
    rsync -av --inplace ${SNAPSHOT_URL}/ ${DATA_DIR}/
    rsync -av --inplace ${SNAPSHOT_URL}/ ${DATA_DIR}/
    set -e

    # Mark data dir as initialized
    touch ${TEST_FILE}
  init_from_gcs.sh: |-
    #!/usr/bin/env sh
    set -ex # -e exits on error

    # env required
    # S3_ENDPOINT_URL # f.e. "https://storage.googleapis.com"
    # AWS_ACCESS_KEY_ID
    # AWS_SECRET_ACCESS_KEY

    DATA_DIR="/data"
    CHAINDATA_DIR="${DATA_DIR}/geth/chaindata"
    INITIALIZED_FILE="${DATA_DIR}/.initialized"
    #without gs:// or s3://, just a bucket name and path
    INDEX_URL="bucket/path/to/file"
    S5CMD=/s5cmd
    EXCLUDE_ANCIENT='--exclude "*.cidx" --exclude "*.ridx" --exclude "*.cdat" --exclude "*.rdat"'
    EXCLUDE_STATE='--exclude "*.ldb"'
    INDEX="index"
    S_UPDATING="/updating"
    S_TIMESTAMP="/timestamp"
    S_STATE_URL="/state_url"
    S_ANCIENT_URL="/ancient_url"
    S_STATS="/stats"

    if [ -f "${INITIALIZED_FILE}" ]; then
        echo "Blockchain already initialized. Exiting..."
        exit 0
    fi

    # we need to create temp files
    cd /tmp

    # get index of source base dirs
    ${S5CMD} cp "s3://${INDEX_URL}" "${INDEX}"

    # get the most fresh datadir
    # prune time is ignored here, we assume that all datadirs are pruned frequently enough
    GCS_BASE_URL=""
    MAX_TIMESTAMP=1
    for _GCS_BASE_URL in $(cat ${INDEX});do
      _TIMESTAMP_URL="${_GCS_BASE_URL}${S_TIMESTAMP}"
      _TIMESTAMP=$(${S5CMD} cat s3://${_TIMESTAMP_URL})
      if [ "${_TIMESTAMP}" -gt "${MAX_TIMESTAMP}" ];then
        GCS_BASE_URL="${_GCS_BASE_URL}"
        MAX_TIMESTAMP=${_TIMESTAMP}
      fi
    done

    if [ "${GCS_BASE_URL}" == "" ];then
      echo "Fatal: cannot pick up correct base url, exiting"
      exit 1
    fi


    UPDATING_URL="${GCS_BASE_URL}${S_UPDATING}"
    TIMESTAMP_URL="${GCS_BASE_URL}${S_TIMESTAMP}"
    STATS_URL="${GCS_BASE_URL}${S_STATS}"

    # get state and ancient sources
    STATE_URL="${GCS_BASE_URL}${S_STATE_URL}"
    ANCIENT_URL="${GCS_BASE_URL}${S_ANCIENT_URL}"


    STATE_SRC="$(${S5CMD} cat s3://${STATE_URL})"
    ANCIENT_SRC="$(${S5CMD} cat s3://${ANCIENT_URL})"
    REMOTE_STATS="$(${S5CMD} cat s3://${STATS_URL})"

    # create dst dirs
    mkdir -p "${CHAINDATA_DIR}/ancient"

    # save sync source
    echo "${GCS_BASE_URL}" > "${DATA_DIR}/source"

    set +e

    # some background monitoring for humans
    set +x
    while true;do
      INODES=$(df -Phi ${DATA_DIR} | tail -n 1 | awk '{print $3}')
      SIZE=$(df -P -BG ${DATA_DIR} | tail -n 1 | awk '{print $3}')G
      echo -e "$(date -Iseconds) | SOURCE TOTAL ${REMOTE_STATS} | DST USED Inodes:\t${INODES} Size:\t${SIZE}"
      sleep 2
    done &
    MON_PID=$!
    set -x

    # get start and stop timestamps from the cloud
    UPDATING_0="$(${S5CMD} cat s3://${UPDATING_URL})"
    TIMESTAMP_0="$(${S5CMD} cat s3://${TIMESTAMP_URL})"


    # we're ready to perform actual data sync

    # we're done when both are true
    # 1) start and stop timestamps did not changed during data sync - no process started or finished updating the cloud
    # 2) 0 objects copied
    SYNC=2
    CLEANUP=1
    while [ "${SYNC}" -gt 0 ] ; do

        # Cleanup
        if [ ${CLEANUP} -eq 1 ];then
          echo "$(date -Iseconds) Cleaning up local dir ..."
          mkdir -p ${DATA_DIR}/geth
          mv ${DATA_DIR}/geth ${DATA_DIR}/geth.old && rm -rf ${DATA_DIR}/geth.old &
          CLEANUP=0
        fi

        # sync from cloud to local disk, without removing existing [missing in the cloud] files
        # run multiple syncs in background

        # we don't wanna sync ancient data here
        time ${S5CMD} cp -n -s -u ${EXCLUDE_ANCIENT} s3://${STATE_SRC}/* ${CHAINDATA_DIR}/ > cplist_state.txt &
        STATE_CP_PID=$!
        time nice ${S5CMD} cp -n -s -u --part-size 200 --concurrency 2 ${EXCLUDE_STATE} s3://${ANCIENT_SRC}/* ${CHAINDATA_DIR}/ancient/ > cplist_ancient.txt &
        ANCIENT_CP_PID=$!

        # wait for all syncs to complete
        # TODO any errors handling here?
        wait ${STATE_CP_PID} ${ANCIENT_CP_PID}

        # get start and stop timestamps from the cloud after sync
        UPDATING_1="$(${S5CMD} cat s3://${UPDATING_URL})"
        TIMESTAMP_1="$(${S5CMD} cat s3://${TIMESTAMP_URL})"

        # compare timestamps before and after sync
        if [ "${UPDATING_0}" -eq "${UPDATING_1}" ] && [ "${TIMESTAMP_0}" -eq "${TIMESTAMP_1}" ];then
          echo "Timestamps are equal"
          echo -e "U_0=${UPDATING_0}\tU_1=${UPDATING_1},\tT_0=${TIMESTAMP_0}\tT_1=${TIMESTAMP_1}"
          let SYNC=SYNC-1
        else
          echo "Timestamps changed, running sync again ..."
          echo -e "U_0=${UPDATING_0}\tU_1=${UPDATING_1},\tT_0=${TIMESTAMP_0}\tT_1=${TIMESTAMP_1}"
          # end  timestamps -> begin timestamps
          UPDATING_0=${UPDATING_1}
          TIMESTAMP_0=${TIMESTAMP_1}
          SYNC=2
          # hack until we resolve sync up without full destination cleanup
          CLEANUP=1
          continue
        fi

        # stop monitoring
        if [ ${MON_PID} -ne 0 ];then
          kill ${MON_PID}
          MON_PID=0
        fi

        # get number of objects copied
        CP_OBJ_NUMBER_STATE=$(wc -l <  cplist_state.txt )
        CP_OBJ_NUMBER_ANCIENT=$(wc -l < cplist_ancient.txt )
        #  0 objects copied ?
        if [ "${CP_OBJ_NUMBER_STATE}" -eq 0 ] && [ "${CP_OBJ_NUMBER_ANCIENT}" -eq 0 ];then
          echo -e "State objects copied:\t${CP_OBJ_NUMBER_STATE}, ancient objects copied:\t${CP_OBJ_NUMBER_ANCIENT}"
          let SYNC=SYNC-1
        else
          echo -e "State objects copied:\t${CP_OBJ_NUMBER_STATE}, ancient objects copied:\t${CP_OBJ_NUMBER_ANCIENT}, running sync again ... "
          SYNC=2
          continue
        fi
    done

    # Mark data dir as initialized
    touch ${INITIALIZED_FILE}
  sync_to_gcs.sh: |-
    #!/usr/bin/env sh
    set -ex # -e exits on error

    # env required
    # S3_ENDPOINT_URL # f.e. "https://storage.googleapis.com"
    # AWS_ACCESS_KEY_ID
    # AWS_SECRET_ACCESS_KEY
    # SYNC_TO_GCS True or any other value

    # enable sync via env
    if [ "${SYNC_TO_GCS}" != "True" ];then
      exit 0
    fi

    DATA_DIR="/data"
    CHAINDATA_DIR="${DATA_DIR}/geth/chaindata"
    #without gs:// or s3://, just a bucket name and path
    GCS_BASE_URL="bucket/path/to/dir"
    # GSUTIL=$(which gsutil)

    S5CMD=/s5cmd
    CPLOG="${DATA_DIR}/cplog.txt"
    CPLIST="${DATA_DIR}/cplist.txt"
    RMLOG="${DATA_DIR}/rmlog.txt"
    RMLIST="${DATA_DIR}/rmlist.txt"

    # s5cmd excludes just by file extension, not by file path
    EXCLUDE_ANCIENT='--exclude "*.cidx" --exclude "*.ridx" --exclude "*.cdat" --exclude "*.rdat"'
    EXCLUDE_STATE='--exclude "*.ldb"'

    S_UPDATING="/updating"
    S_TIMESTAMP="/timestamp"
    S_STATE_URL="/state_url"
    S_ANCIENT_URL="/ancient_url"
    S_STATS="/stats"

    if [ "${GCS_BASE_URL}" == "" ];then
      echo "Fatal: cannot use empty base url, exiting"
      exit 1
    fi

    # we need to create temp files
    cd /tmp

    # get timestamp, state and ancient DSTs
    UPDATING_URL="${GCS_BASE_URL}${S_UPDATING}"
    TIMESTAMP_URL="${GCS_BASE_URL}${S_TIMESTAMP}"
    STATS_URL="${GCS_BASE_URL}${S_STATS}"

    STATE_URL="${GCS_BASE_URL}${S_STATE_URL}"
    ANCIENT_URL="${GCS_BASE_URL}${S_ANCIENT_URL}"

    STATE_DST="$(${S5CMD} cat s3://${STATE_URL})"
    ANCIENT_DST="$(${S5CMD} cat s3://${ANCIENT_URL})"

    # mark begin of sync in the cloud
    date +%s > updating
    ${S5CMD} cp updating "s3://${UPDATING_URL}"

    # we're ready to perform actual data copy

    # sync from local disk to cloud, without removing existing [missing on local disk] files
    # run multiple syncs in background
    # cp is recursive by default, thus we need to exclude ancient data here
    time ${S5CMD} cp -n -s -u ${EXCLUDE_ANCIENT} "${CHAINDATA_DIR}/" "s3://${STATE_DST}/"  > cplist_state.txt &
    time nice ${S5CMD} cp -n -s -u --part-size 200 --concurrency 2 ${EXCLUDE_STATE} "${CHAINDATA_DIR}/ancient/" "s3://${ANCIENT_DST}/"  > cplist_ancient.txt &
    # wait for all syncs to complete
    # TODO any errors handling here?
    wait

    # update timestamp
    # TODO store timestamp inside readinnes check and use it instead of now()
    date +%s > timestamp
    ${S5CMD} cp timestamp "s3://${TIMESTAMP_URL}"

    # update stats
    INODES=$(df -Phi "${DATA_DIR}" | tail -n 1 | awk '{print $3}')
    # force GB output
    SIZE=$(df -P -BG "${DATA_DIR}" | tail -n 1 | awk '{print $3}')G
    echo -ne "Inodes:\t${INODES} Size:\t${SIZE}" > stats
    ${S5CMD} cp stats "s3://${STATS_URL}"

    # get number of objects copied
    cat cplist_state.txt cplist_ancient.txt > "${CPLIST}"
    # we use a heuristic here - lot of uploaded objects => lot of object to remove in the cloud => we need to generate removal list
    CP_OBJ_NUMBER=$(wc -l < "${CPLIST}")
    echo "$(date -Iseconds) Uploaded objects: ${CP_OBJ_NUMBER}" | tee -a "${CPLOG}"
    set +e
    if [ "${CP_OBJ_NUMBER}" -gt 1000 ] ;then
      set -e
      # s5cmd doesn't support GCS object removal, just generate a list of files to remove via gsutil
      # removal should be done in another sidecar
      time $S5CMD --dry-run cp -n ${EXCLUDE_ANCIENT} "s3://${STATE_DST}/*" "${CHAINDATA_DIR}/" | awk '{print $2}'|sed 's/^s3/gs/' > rmlist.txt
      time $S5CMD --dry-run cp -n ${EXCLUDE_STATE} "s3://${ANCIENT_DST}/*" "${CHAINDATA_DIR}/ancient/" | awk '{print $2}'|sed 's/^s3/gs/' >> rmlist.txt
      echo "$(date -Iseconds) Objects to remove: $(wc -l < rmlist.txt)" | tee -a "${RMLOG}"
      cp rmlist.txt "${RMLIST}"
    fi
  gcs_cleanup.sh: |-
    #!/usr/bin/env sh
    set -ex

    # env required
    # AWS_ACCESS_KEY_ID
    # AWS_SECRET_ACCESS_KEY

    DATA_DIR="/data"
    RMLIST="${DATA_DIR}/rmlist.txt"
    RMLOG="${DATA_DIR}/rmlog.txt"
    GSUTIL="/google-cloud-sdk/bin/gsutil"
    GCLOUD="/google-cloud-sdk/bin/gcloud"
    BOTOCONFIG="${HOME}/.boto"

    # allow container interrupt
    trap "{ exit 1; }" INT TERM

    # disable gcloud auth
    ${GCLOUD} config set pass_credentials_to_gsutil false
    # using env vars to auth to GCS, as we use these env vars for s5cmd already
    set +x
    echo -e "[Credentials]\ngs_access_key_id = ${AWS_ACCESS_KEY_ID}\ngs_secret_access_key = ${AWS_SECRET_ACCESS_KEY}" > "${BOTOCONFIG}"
    set -x

    # creating empty list if missing, f.e .no sync-to-gcs were running
    touch "${RMLIST}"

    OBJ_TO_REMOVE=$(wc -l < "${RMLIST}")
    set +e
    if [ "${OBJ_TO_REMOVE}" -gt "10000" ];then
      split -l 2000 "${RMLIST}" /tmp/rmlist-
      find /tmp -name "rmlist-*" -print0| xargs -0 -P 50 -n1 -I{} bash -c "nice ${GSUTIL} rm -f -I < {}"
      echo "$(date -Iseconds) Removed objects: ${OBJ_TO_REMOVE}" | tee -a "${RMLOG}"
    else
      if [ "${OBJ_TO_REMOVE}" -gt "0" ];then
        time ${GSUTIL} -m rm -I < "${RMLIST}"
        echo "$(date -Iseconds) Removed objects: ${OBJ_TO_REMOVE}" | tee -a "${RMLOG}"
      else
        echo "No objects to remove"
      fi
    fi

    # cleanup removal list
    true > "${RMLIST}"

    # sleep in an endless cycle to allow container interrupt
    set +x
    while true; do sleep 10;done
  prune.sh: |-
    #!/usr/bin/env sh

    set -x
    # env required
    # BSC_PRUNE = True or any other value

    DATA_DIR="/data"

    # TODO
    # mark cloud timestamp as outdated, as we're not going to provide a fresh snapshot soon
    # it should be done by operator - just keep pod up & running for some time and cloud timestamp will lag on it's own

    GETH=/usr/local/bin/geth

    ret=0
    # we need to check env var to start pruning
    if [ "${BSC_PRUNE}" == "True" ] ; then
      # background logging
      tail -F "${DATA_DIR}/bsc.log" &
      $GETH --config=/config/config.toml --datadir=${DATA_DIR} --cache 8192 snapshot prune-state
      # prune-block will turn our full node into light one actually
      # $GETH --config=/config/config.toml --datadir=${DATA_DIR} --datadir.ancient=${DATA_DIR}/geth/chaindata/ancient --cache 8192 snapshot prune-block
      ret=$?
      if [ "${ret}" -eq "0" ];then
        # update timestamp
        date +%s > "${DATA_DIR}/prune_timestamp"
      fi
    fi

    exit $ret
  prune_block.sh: |-
    #!/usr/bin/env sh

    # script performs BSC block prune from ancientDB, removing [unneeded in some cases] historical blocks
    # good use case is a bootnode w/o RPC
    # block prune should speed up node in runtime as well as allow to use less disk space
    # after block prune the node will not be able to serve RPC on pruned blocks
    # Around last 90000 blocks are kept in the node state before moving on to ancientDB

    set -x

    DATA_DIR="/data"

    GETH=/usr/local/bin/geth
    # how much recent blocks do we need to keep. Default 0 means we clean up ancientDB completely
    BLOCKS_RESERVED=${1:-0}

    ret=0
      # background logging
      tail -F "${DATA_DIR}/bsc.log" &
      # prune-block will turn our full node into light one actually
      $GETH --config=/config/config.toml --datadir=${DATA_DIR} --datadir.ancient=${DATA_DIR}/geth/chaindata/ancient --cache 8192 snapshot prune-block --block-amount-reserved=${BLOCKS_RESERVED}
      ret=$?
      if [ "${ret}" -eq "0" ];then
        # update timestamp
        date +%s > "${DATA_DIR}/block_prune_timestamp"
      fi

    exit $ret
  cron.sh: |-
    #!/usr/bin/env sh

    # possible values are
    # "sync" to trigger sync-to-gcs
    # "prune" to trigger prune
    MODE="${1}"
    POD_NAME=release-name-bsc-0
    CONFIGMAP_NAME="release-name-env"
    KUBECTL=$(which kubectl)
    # wait timeout, f.e "30s"
    WAIT_TIMEOUT="${2}"
    PATCH_DATA=""

    check_ret(){
            ret="${1}"
            msg="${2}"
            # allow to override exit code, default value is ret
            exit_code=${3:-${ret}}
            if [ ! "${ret}" -eq 0 ];then
                    echo "${msg}"
                    echo "return code ${ret}, exit code ${exit_code}"
                    exit "${exit_code}"
            fi
    }

    # input data checking
    [ "${MODE}" = "prune" ] || [ "${MODE}" = "sync" ]
    check_ret $? "$(date -Iseconds) Mode value \"${MODE}\" is incorrect, abort"

    # wait for pod to become ready
    echo "$(date -Iseconds) Waiting ${WAIT_TIMEOUT} for pod ${POD_NAME} to become ready ..."
    ${KUBECTL} wait --timeout="${WAIT_TIMEOUT}" --for=condition=Ready pod "${POD_NAME}"
    ret=$?
    # exit code override to "success"
    check_ret "${ret}" "$(date -Iseconds) Pod ${POD_NAME} is not ready, nothing to do, exiting" 0

    # ensuring pod is not terminating now
    # https://github.com/kubernetes/kubernetes/issues/22839
    echo "$(date -Iseconds) Checking for pod ${POD_NAME} to not terminate ..."
    DELETION_TIMESTAMP=$(${KUBECTL} get -o jsonpath='{.metadata.deletionTimestamp}' pod "${POD_NAME}")
    ret=$?
    check_ret "${ret}" "$(date -Iseconds) Cannot get pod ${POD_NAME}, abort"

    # empty timestamp means that pod is not terminating now
    if [ "${DELETION_TIMESTAMP}" = "" ];then
      # we're good to go
      echo "$(date -Iseconds) Pod ${POD_NAME} is ready, continuing"

      case "${MODE}" in
      "sync")
        echo "$(date -Iseconds) Patching configmap ${CONFIGMAP_NAME} to enable sync and disable prune"
        # disable prune, enable sync-to-gcs
        PATCH_DATA='{"data":{"BSC_PRUNE":"False","SYNC_TO_GCS":"True"}}'
        ;;
      "prune")
        echo "$(date -Iseconds) Patching configmap ${CONFIGMAP_NAME} to enable prune and disable sync"
        # disable sync-to-gcs, enable prune
        PATCH_DATA='{"data":{"BSC_PRUNE":"True","SYNC_TO_GCS":"False"}}'
        ;;
      "*")
         check_ret 1 "$(date -Iseconds) Mode value \"${MODE}\" is incorrect, abort"
         ;;
      esac
      ${KUBECTL} patch configmap "${CONFIGMAP_NAME}"  --type merge --patch ${PATCH_DATA}
      ret=$?
      check_ret "${ret}" "$(date -Iseconds) Fatal: cannot patch configmap ${CONFIGMAP_NAME}, abort"

      echo "$(date -Iseconds) Deleting pod ${POD_NAME} to trigger action inside init container ..."
      # delete the pod to trigger action inside init container
      ${KUBECTL} delete pod "${POD_NAME}" --wait=false
      ret=$?
      check_ret "${ret}" "$(date -Iseconds) Fatal: cannot delete pod ${POD_NAME}, abort"
      echo "$(date -Iseconds) Pod ${POD_NAME} deleted successfully, exiting. Check pod logs after respawn."
    else
      # pod is terminating now, try later on next iteration
      echo "$(date -Iseconds) Pod ${POD_NAME} is terminating now, nothing to do, exiting"
      exit 0
    fi
  update-pod-deletion-cost.sh: |-
    #!/usr/bin/env sh

    # this script updates "pod-deletion-cost" pod annotation based on disk usage

    # required env
    # MY_POD_NAME - pod name to annotate

    # get used space from this dir's mount point
    DATA_DIR="${1}"
    # sleep between annotate iterations, in 10*second
    INTERVAL="${2}"

    KUBECTL=$(which kubectl)
    ANNOTATION=""

    # allow container interrupts
    trap "{ exit 1; }" INT TERM

    while [ true ]; do
      # get dir's mount point usage in MB
      SIZE=$(df -P -BM "${DATA_DIR}" | tail -n 1 | awk '{print $3}'|sed 's/M//g')
      # use negative values, the bigger is the disk usage the lower is the pod deletion cost
      # thus the most "heavy" pod will be removed first
      ANNOTATION="controller.kubernetes.io/pod-deletion-cost=-${SIZE}"
      ${KUBECTL} annotate --overwrite=true pod "${MY_POD_NAME}" "${ANNOTATION}"
      ret=$?
      if [ ${ret} -eq 0 ];then
        echo "$(date -Iseconds) Annotated pod ${MY_POD_NAME} with ${ANNOTATION}"
      else
        echo "$(date -Iseconds) Error annotating pod ${MY_POD_NAME} with ${ANNOTATION}"
      fi
      # we need to sleep <short-delay> inside cycle to handle pod termination w/o delays
      for i in $(seq 2 "${INTERVAL}");do sleep 10;done
    done
  controller-hook.sh: |-
    #!/usr/bin/env bash

    # This is needed to work-around GKE local SSD scale up issue https://github.com/kubernetes/autoscaler/issues/2145

    # watch for scale-related changes in the specified controller
    # in scale up case -> scale up secondary controller to the same replica value
    # in scale down case -> scale down secondary controller to 0

    controller=StatefulSet
    apiVersion="apps/v1"

    watchName="OnModified${controller}"
    debugLog="/tmp/debug.log"

    if [[ $1 == "--config" ]] ; then
      cat <<EOF
    configVersion: v1
    kubernetes:
    - name: "${watchName}"
      apiVersion: ${apiVersion}
      kind: $controller
      executeHookOnEvent: ["Modified"]
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: bsc
          app.kubernetes.io/instance: release-name
      namespace:
        nameSelector:
          matchNames: ["default"]
      jqFilter: ".spec.replicas"
    EOF
    else
    	# ignore Synchronization for simplicity
    	type=$(jq -r '.[0].type' "${BINDING_CONTEXT_PATH}")
    	if [[ "${type}" == "Synchronization" ]] ; then
    	  echo Got Synchronization event
    	  exit 0
    	fi
      	ARRAY_COUNT=$(jq -r '. | length-1' "${BINDING_CONTEXT_PATH}")
    	for i in $(seq 0 "${ARRAY_COUNT}");do
    		bindingName=$(jq -r ".[$i].binding" "${BINDING_CONTEXT_PATH}")
    		resourceEvent=$(jq -r ".[$i].watchEvent" "${BINDING_CONTEXT_PATH}")
    		resourceName=$(jq -r ".[$i].object.metadata.name" "${BINDING_CONTEXT_PATH}")
    		if [[ "${bindingName}" == "${watchName}" && "${resourceEvent}" == "Modified" ]] ; then
    		  	echo "${controller} ${resourceName} was scaled"
    		  	newReplicas=$(jq -r ".[$i].object.spec.replicas" "${BINDING_CONTEXT_PATH}")
            oldReplicas=$(jq -r ".[$i].object.status.replicas" "${BINDING_CONTEXT_PATH}")
            addController=$(jq -r ".[$i].object.metadata.annotations.additionalControllerName" "${BINDING_CONTEXT_PATH}")
            namespace=$(jq -r ".[$i].object.metadata.namespace" "${BINDING_CONTEXT_PATH}")
            if [[ "${newReplicas}" -gt "${oldReplicas}" ]];then
              echo "$(date -Iseconds) scale UP ${controller} ${addController}, new=${newReplicas}, old=${oldReplicas}" | tee -a "${debugLog}"
              kubectl --namespace "${namespace}" scale "${controller}" "${addController}" --replicas="${newReplicas}"
            fi
            if [[ "${newReplicas}" -lt "${oldReplicas}" ]];then
              echo "$(date -Iseconds) scale DOWN ${controller} ${addController} to 0, new=${newReplicas}, old=${oldReplicas}" | tee -a "${debugLog}"
              kubectl --namespace "${namespace}" scale "${controller}" "${addController}" --replicas=0
            fi
    		fi
    	done
    #	echo  "$(date -Iseconds)"  >> "${debugLog}"
    #	cat "${BINDING_CONTEXT_PATH}" | jq . >> "${debugLog}"
    fi
  rotate.sh: |-
    #!/usr/bin/env bash

    # this script rotates bootnodes - replaces old node with a new one
    # scale up a new controller
    # wait a couple hours for bootnode to grab some p2p peers
    # ensure node is up & synced using JSON RPC via k8s service
    # scale down an old controller

    # We have to use 1 controller per bootnode due to external static IP and static node ID

    # f.e statefulset/bootnode-0 or cloneset/bootnode-1
    NEW_CONTROLLER="${1}"
    #
    OLD_CONTROLLER="${2}"
    # bsc rpc endpoint to check, f.e. bsc-bootnode-0:8575, where bsc-bootnode-0 is a k8s service name pointing to a single node
    RPC_ENDPOINT="${3}"
    # node is allowed to lag for seconds
    MAX_NODE_LAG=120
    # wait for the new node to spin up and sync up
    # ensure that cronjob's activeDeadlineSeconds is greater than this value
    WAIT_TIME=7200

    KUBECTL=$(which kubectl)
    CURL=$(which curl)
    JQ=$(which jq)

    check_ret(){
            ret="${1}"
            msg="${2}"
            # allow to override exit code, default value is ret
            exit_code=${3:-${ret}}
            if [ ! "${ret}" -eq 0 ];then
                    echo "${msg}"
                    echo "return code ${ret}, exit code ${exit_code}"
                    exit "${exit_code}"
            fi
    }

    # spinning up a new node
    echo "$(date -Iseconds) scaling ${NEW_CONTROLLER} up to 1"
    ${KUBECTL} scale "${NEW_CONTROLLER}" --replicas=1
    check_ret $? "$(date -Iseconds) FATAL: cannot scale ${NEW_CONTROLLER} to 1 replica" 1

    # wait for a new node to sync up
    echo "$(date -Iseconds) sleeping ${WAIT_TIME} seconds"
    sleep ${WAIT_TIME}
    echo "$(date -Iseconds) check node readinnes"

    # health check
    # cannot use existing check_node_readiness.sh as there is no bsc binary in kubectl docker image
    # get latest block and parse it's timestamp via jq
    JSON_RPC_REQUEST='{"jsonrpc":"2.0","method":"eth_getBlockByNumber","params":["latest",false],"id":1}'
    LATEST_BLOCK_TIMESTAMP_HEX=$(${CURL} -s --data-binary ${JSON_RPC_REQUEST} -H 'Content-Type: application/json' "${RPC_ENDPOINT}"|${JQ} -r .result.timestamp)
    [ -n "${LATEST_BLOCK_TIMESTAMP_HEX}" ]
    check_ret $? "$(date -Iseconds) FATAL: node ${NEW_CONTROLLER} did not pass the health check - empty LATEST_BLOCK_TIMESTAMP_HEX" 2
    echo "$(date -Iseconds) Latest block timestamp hex ${LATEST_BLOCK_TIMESTAMP_HEX}"

    # convert timestamp from hex to dec
    LATEST_BLOCK_TIMESTAMP=$(printf '%d' "${LATEST_BLOCK_TIMESTAMP_HEX}")
    [ -n "${LATEST_BLOCK_TIMESTAMP}" ]
    check_ret $? "$(date -Iseconds) FATAL: node ${NEW_CONTROLLER} did not pass the health check - empty LATEST_BLOCK_TIMESTAMP" 3
    echo "$(date -Iseconds) Latest block timestamp ${LATEST_BLOCK_TIMESTAMP}"

    # is node synced up ?
    [[ $(($(date +%s) - ${LATEST_BLOCK_TIMESTAMP})) -le ${MAX_NODE_LAG} ]]
    check_ret $? "$(date -Iseconds) FATAL: node ${NEW_CONTROLLER} timestamp lag is greater than ${MAX_NODE_LAG}, ts=${LATEST_BLOCK_TIMESTAMP}, now=$(date +%s)" 4
    echo "$(date -Iseconds) node ${NEW_CONTROLLER} timestamp ${LATEST_BLOCK_TIMESTAMP} is fresh, now=$(date +%s)"

    # scaling down an old node
    echo "$(date -Iseconds) scaling ${OLD_CONTROLLER} down to 0"
    ${KUBECTL} scale "${OLD_CONTROLLER}" --replicas=0
    check_ret $? "$(date -Iseconds) FATAL: cannot scale ${OLD_CONTROLLER} to 0 replica" 5
    exit 0
---
apiVersion: v1
kind: Service
metadata:
  name: release-name
  labels:
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.10
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: v1.1.8
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8575
      name: jsonrpc
      protocol: TCP
    - port: 8576
      name: web-socket
      protocol: TCP
    - port: 8577
      name: qraphql
      protocol: TCP
    - port: 30311
      name: p2p
      protocol: TCP
    - port: 30311
      name: p2p-discovery
      protocol: UDP
    - port: 9368
      name: metrics
      protocol: TCP
    - port: 6060
      name: bsc-metrics
      protocol: TCP
      targetPort: 6060
  selector:
    app.kubernetes.io/name: bsc
    app.kubernetes.io/instance: release-name
    manualstatus: in-service
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-bsc
  labels:
    app.kubernetes.io/name: bsc
    helm.sh/chart: bsc-0.6.10
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: v1.1.8
    app.kubernetes.io/managed-by: Helm
  annotations: null
spec:
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  serviceName: release-name-service
  podManagementPolicy: OrderedReady
  selector:
    matchLabels:
      app.kubernetes.io/name: bsc
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: bsc
        app.kubernetes.io/instance: release-name
        bsc/chain: mainnet
        bsc/role: rpc
        manualstatus: in-service
      annotations:
        checksum/config: 83b9ad4b1cec55f9fa8ee3f42487964d78ba4739b5690d468fd05004e60cf465
    spec:
      serviceAccountName: release-name-bsc
      securityContext:
        fsGroup: 101
        runAsGroup: 101
        runAsUser: 101
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: bsc
                    bsc/chain: mainnet
                topologyKey: failure-domain.beta.kubernetes.io/zone
              weight: 100
      terminationGracePeriodSeconds: 180
      containers:
        - name: bsc
          image: dysnix/bsc:latest
          imagePullPolicy: Always
          args:
            - --config=/config/config.toml
            - --datadir=/data
            - --syncmode=full
            - --gcmode=full
            - --maxpeers=50
            - --cache=8192
            - --snapshot=false
            - --pipecommit=false
            - --persistdiff=false
            - --diffblock=86400
            - --port=30311
            - --rpc.allow-unprotected-txs
            - --txlookuplimit=0
            - --cache.preimages
            - --diffsync
            - --metrics
            - --pprof
            - --pprof.addr=0.0.0.0
            - --pprof.port=6060
          envFrom:
            - configMapRef:
                name: release-name-env
          workingDir: /data
          resources: null
          ports:
            - containerPort: 8575
              name: jsonrpc
              protocol: TCP
            - containerPort: 8576
              name: web-socket
              protocol: TCP
            - containerPort: 8577
              name: qraphql
              protocol: TCP
            - containerPort: 30311
              hostPort: 30311
              name: p2p
              protocol: TCP
            - containerPort: 30311
              hostPort: 30311
              name: p2p-discovery
              protocol: UDP
            - containerPort: 9368
              name: metrics
              protocol: TCP
          volumeMounts:
            - name: generated-bsc-config
              mountPath: /config
            - name: scripts
              mountPath: /scripts
            - name: probe-env
              mountPath: /env
            - name: bsc-pvc
              mountPath: /data
          startupProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - source /env/env.txt && /bin/sh /scripts/check_node_health.sh http://127.0.0.1:8575 $StartupProbeTimestampDistinct last_synced_block.txt
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 12
        - name: logger
          image: krallin/ubuntu-tini:latest
          imagePullPolicy: IfNotPresent
          args:
            - tail
            - -F
            - /data/bsc.log
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - sleep 30
          workingDir: /data
          volumeMounts:
            - name: bsc-pvc
              mountPath: /data
              readOnly: true
      initContainers:
        - name: remove-lock
          command:
            - rm
            - -f
            - /data/geth/LOCK
          image: busybox
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: bsc-pvc
              mountPath: /data
        - name: generate-bsc-config
          command:
            - sh
            - /scripts/generate_node_config.sh
          env:
            - name: GENERATE_CONFIG
              value: "false"
            - name: HOME
              value: /tmp
          image: google/cloud-sdk:alpine
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: bsc-config
              mountPath: /config
            - name: generated-bsc-config
              mountPath: /generated-config
            - name: scripts
              mountPath: /scripts
      volumes:
        - name: bsc-config
          configMap:
            name: release-name-config
        - name: generated-bsc-config
          emptyDir: {}
        - name: scripts
          configMap:
            name: release-name-scripts
        - name: probe-env
          configMap:
            name: release-name-probe-env
  volumeClaimTemplates:
    - metadata:
        name: bsc-pvc
        labels:
          app: bsc
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1000Gi
        volumeMode: Filesystem
---
null
---
null
