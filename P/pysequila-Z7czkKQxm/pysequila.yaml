apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: hub
      app: jupyterhub
      release: release-name
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: proxy
  labels:
    component: proxy
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: proxy
      app: jupyterhub
      release: release-name
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: user-placeholder
  labels:
    component: user-placeholder
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  minAvailable: 0
  selector:
    matchLabels:
      component: user-placeholder
      app: jupyterhub
      release: release-name
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      component: user-scheduler
      app: jupyterhub
      release: release-name
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
---
kind: Secret
apiVersion: v1
metadata:
  name: hub-secret
  labels:
    component: hub
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
type: Opaque
data:
  proxy.token: Y2hhbmdlbWU=
  values.yaml: YXV0aDoge30KaHViOgogIHNlcnZpY2VzOiB7fQ==
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: hub-config
  labels:
    component: hub
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
data:
  values.yaml: |
    Chart:
      Name: jupyterhub
      Version: 0.9.1
    Release:
      Name: release-name
      Namespace: default
      Service: Helm
    auth:
      admin:
        access: true
      dummy: {}
      ldap:
        dn:
          search: {}
          user: {}
        user: {}
      state:
        enabled: false
      type: dummy
      whitelist: {}
    cull:
      concurrency: 10
      enabled: true
      every: 600
      maxAge: 0
      removeNamedServers: false
      timeout: 3600
      users: false
    custom: {}
    debug:
      enabled: false
    hub:
      allowNamedServers: false
      annotations: {}
      baseUrl: /
      concurrentSpawnLimit: 64
      consecutiveFailureLimit: 5
      db:
        pvc:
          accessModes:
          - ReadWriteOnce
          annotations: {}
          selector: {}
          storage: 1Gi
        type: sqlite-pvc
      deploymentStrategy:
        type: Recreate
      extraConfig: {}
      extraContainers: []
      extraVolumeMounts: []
      extraVolumes: []
      fsGid: 1000
      image:
        name: jupyterhub/k8s-hub
        tag: 0.9.1
      imagePullSecret:
        enabled: false
      initContainers: []
      labels: {}
      livenessProbe:
        enabled: false
        initialDelaySeconds: 30
        periodSeconds: 10
      networkPolicy:
        egress:
        - to:
          - ipBlock:
              cidr: 0.0.0.0/0
        enabled: false
        ingress: []
      nodeSelector: {}
      pdb:
        enabled: true
        minAvailable: 1
      readinessProbe:
        enabled: true
        initialDelaySeconds: 0
        periodSeconds: 10
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
      service:
        annotations: {}
        ports: {}
        type: ClusterIP
      services: {}
      templatePaths: []
      templateVars: {}
      uid: 1000
    scheduling:
      corePods:
        nodeAffinity:
          matchNodePurpose: prefer
      podPriority:
        defaultPriority: 0
        enabled: false
        globalDefault: false
        userPlaceholderPriority: -10
      userPlaceholder:
        enabled: true
        replicas: 0
      userPods:
        nodeAffinity:
          matchNodePurpose: prefer
      userScheduler:
        enabled: true
        image:
          name: gcr.io/google_containers/kube-scheduler-amd64
          tag: v1.13.12
        logLevel: 4
        nodeSelector: {}
        pdb:
          enabled: true
          minAvailable: 1
        policy: {}
        replicas: 2
        resources:
          requests:
            cpu: 50m
            memory: 256Mi
    singleuser:
      cloudMetadata:
        enabled: false
        ip: 169.254.169.254
      cmd: jupyterhub-singleuser
      cpu: {}
      defaultUrl: /lab
      events: true
      extraAnnotations: {}
      extraContainers: []
      extraEnv:
        HOME: /home/jovyan
        PYSEQUILA_VERSION: 0.1.6
        SEQUILA_VERSION: 0.5.16
        TMP_HOME: /tmp/jovyan
      extraLabels:
        hub.jupyter.org/network-access-hub: "true"
      extraNodeAffinity:
        preferred: []
        required: []
      extraPodAffinity:
        preferred: []
        required: []
      extraPodAntiAffinity:
        preferred: []
        required: []
      extraPodConfig: {}
      extraResource:
        guarantees: {}
        limits: {}
      extraTolerations: []
      fsGid: 100
      image:
        name: biodatageeks/pysequila-notebook
        pullPolicy: IfNotPresent
        tag: 0.1.3-ga5af501
      imagePullSecret:
        enabled: false
      initContainers: []
      lifecycleHooks:
        postStart:
          exec:
            command:
            - sh
            - -c
            - |
              mkdir -p $HOME/.local/share/jupyter/kernels/; cp -r $TMP_HOME/venv $HOME; cp -r $TMP_HOME/.sdkman $HOME; cp -r  $TMP_HOME/.local/share/jupyter/kernels/pysequila/ $HOME/.local/share/jupyter/kernels/;
      memory:
        guarantee: 1G
      networkPolicy:
        egress:
        - to:
          - ipBlock:
              cidr: 0.0.0.0/0
              except:
              - 169.254.169.254/32
        enabled: false
        ingress: []
      networkTools:
        image:
          name: jupyterhub/k8s-network-tools
          tag: 0.9.1
      nodeSelector: {}
      serviceAccountName: null
      startTimeout: 300
      storage:
        capacity: 10Gi
        dynamic:
          pvcNameTemplate: claim-{username}{servername}
          storageAccessModes:
          - ReadWriteOnce
          storageClass: standard
          volumeNameTemplate: volume-{username}{servername}
        extraLabels: {}
        extraVolumeMounts: []
        extraVolumes: []
        homeMountPath: /home/jovyan
        static:
          subPath: '{username}'
        type: dynamic
      uid: 1000
  cull_idle_servers.py: |
    #!/usr/bin/env python3
    # Imported from https://github.com/jupyterhub/jupyterhub/blob/6b1046697/examples/cull-idle/cull_idle_servers.py
    """script to monitor and cull idle single-user servers

    Caveats:

    last_activity is not updated with high frequency,
    so cull timeout should be greater than the sum of:

    - single-user websocket ping interval (default: 30s)
    - JupyterHub.last_activity_interval (default: 5 minutes)

    You can run this as a service managed by JupyterHub with this in your config::


        c.JupyterHub.services = [
            {
                'name': 'cull-idle',
                'admin': True,
                'command': 'python cull_idle_servers.py --timeout=3600'.split(),
            }
        ]

    Or run it manually by generating an API token and storing it in `JUPYTERHUB_API_TOKEN`:

        export JUPYTERHUB_API_TOKEN=`jupyterhub token`
        python cull_idle_servers.py [--timeout=900] [--url=http://127.0.0.1:8081/hub/api]
    """

    from datetime import datetime, timezone
    from functools import partial
    import json
    import os

    try:
        from urllib.parse import quote
    except ImportError:
        from urllib import quote

    import dateutil.parser

    from tornado.gen import coroutine, multi
    from tornado.locks import Semaphore
    from tornado.log import app_log
    from tornado.httpclient import AsyncHTTPClient, HTTPRequest
    from tornado.ioloop import IOLoop, PeriodicCallback
    from tornado.options import define, options, parse_command_line


    def parse_date(date_string):
        """Parse a timestamp

        If it doesn't have a timezone, assume utc

        Returned datetime object will always be timezone-aware
        """
        dt = dateutil.parser.parse(date_string)
        if not dt.tzinfo:
            # assume na√Øve timestamps are UTC
            dt = dt.replace(tzinfo=timezone.utc)
        return dt


    def format_td(td):
        """
        Nicely format a timedelta object

        as HH:MM:SS
        """
        if td is None:
            return "unknown"
        if isinstance(td, str):
            return td
        seconds = int(td.total_seconds())
        h = seconds // 3600
        seconds = seconds % 3600
        m = seconds // 60
        seconds = seconds % 60
        return f"{h:02}:{m:02}:{seconds:02}"


    @coroutine
    def cull_idle(
        url,
        api_token,
        inactive_limit,
        cull_users=False,
        remove_named_servers=False,
        max_age=0,
        concurrency=10,
    ):
        """Shutdown idle single-user servers

        If cull_users, inactive *users* will be deleted as well.
        """
        auth_header = {
            'Authorization': 'token %s' % api_token,
        }
        req = HTTPRequest(
            url=url + '/users',
            headers=auth_header,
        )
        now = datetime.now(timezone.utc)
        client = AsyncHTTPClient()

        if concurrency:
            semaphore = Semaphore(concurrency)
            @coroutine
            def fetch(req):
                """client.fetch wrapped in a semaphore to limit concurrency"""
                yield semaphore.acquire()
                try:
                    return (yield client.fetch(req))
                finally:
                    yield semaphore.release()
        else:
            fetch = client.fetch

        resp = yield fetch(req)
        users = json.loads(resp.body.decode('utf8', 'replace'))
        futures = []

        @coroutine
        def handle_server(user, server_name, server):
            """Handle (maybe) culling a single server

            Returns True if server is now stopped (user removable),
            False otherwise.
            """
            log_name = user['name']
            if server_name:
                log_name = '%s/%s' % (user['name'], server_name)
            if server.get('pending'):
                app_log.warning(
                    "Not culling server %s with pending %s",
                    log_name, server['pending'])
                return False

            if server.get('started'):
                age = now - parse_date(server['started'])
            else:
                # started may be undefined on jupyterhub < 0.9
                age = None

            # check last activity
            # last_activity can be None in 0.9
            if server['last_activity']:
                inactive = now - parse_date(server['last_activity'])
            else:
                # no activity yet, use start date
                # last_activity may be None with jupyterhub 0.9,
                # which introduces the 'started' field which is never None
                # for running servers
                inactive = age

            should_cull = (inactive is not None and
                           inactive.total_seconds() >= inactive_limit)
            if should_cull:
                app_log.info(
                    "Culling server %s (inactive for %s)",
                    log_name, format_td(inactive))

            if max_age and not should_cull:
                # only check started if max_age is specified
                # so that we can still be compatible with jupyterhub 0.8
                # which doesn't define the 'started' field
                if age is not None and age.total_seconds() >= max_age:
                    app_log.info(
                        "Culling server %s (age: %s, inactive for %s)",
                        log_name, format_td(age), format_td(inactive))
                    should_cull = True

            if not should_cull:
                app_log.debug(
                    "Not culling server %s (age: %s, inactive for %s)",
                    log_name, format_td(age), format_td(inactive))
                return False

            body = None
            if server_name:
                # culling a named server
                # A named server can be stopped and kept available to the user
                # for starting again or stopped and removed. To remove the named
                # server we have to pass an additional option in the body of our
                # DELETE request.
                delete_url = url + "/users/%s/servers/%s" % (
                    quote(user['name']),
                    quote(server['name']),
                )
                if remove_named_servers:
                    body = json.dumps({"remove": True})
            else:
                delete_url = url + '/users/%s/server' % quote(user['name'])

            req = HTTPRequest(
                url=delete_url,
                method='DELETE',
                headers=auth_header,
                body=body,
                allow_nonstandard_methods=True,
            )
            resp = yield fetch(req)
            if resp.code == 202:
                app_log.warning(
                    "Server %s is slow to stop",
                    log_name,
                )
                # return False to prevent culling user with pending shutdowns
                return False
            return True

        @coroutine
        def handle_user(user):
            """Handle one user.

            Create a list of their servers, and async exec them.  Wait for
            that to be done, and if all servers are stopped, possibly cull
            the user.
            """
            # shutdown servers first.
            # Hub doesn't allow deleting users with running servers.
            # named servers contain the 'servers' dict
            if 'servers' in user:
                servers = user['servers']
            # Otherwise, server data is intermingled in with the user
            # model
            else:
                servers = {}
                if user['server']:
                    servers[''] = {
                        'started': user.get('started'),
                        'last_activity': user['last_activity'],
                        'pending': user['pending'],
                        'url': user['server'],
                    }
            server_futures = [
                handle_server(user, server_name, server)
                for server_name, server in servers.items()
            ]
            results = yield multi(server_futures)
            if not cull_users:
                return
            # some servers are still running, cannot cull users
            still_alive = len(results) - sum(results)
            if still_alive:
                app_log.debug(
                    "Not culling user %s with %i servers still alive",
                    user['name'], still_alive)
                return False

            should_cull = False
            if user.get('created'):
                age = now - parse_date(user['created'])
            else:
                # created may be undefined on jupyterhub < 0.9
                age = None

            # check last activity
            # last_activity can be None in 0.9
            if user['last_activity']:
                inactive = now - parse_date(user['last_activity'])
            else:
                # no activity yet, use start date
                # last_activity may be None with jupyterhub 0.9,
                # which introduces the 'created' field which is never None
                inactive = age

            should_cull = (inactive is not None and
                           inactive.total_seconds() >= inactive_limit)
            if should_cull:
                app_log.info(
                    "Culling user %s (inactive for %s)",
                    user['name'], inactive)

            if max_age and not should_cull:
                # only check created if max_age is specified
                # so that we can still be compatible with jupyterhub 0.8
                # which doesn't define the 'started' field
                if age is not None and age.total_seconds() >= max_age:
                    app_log.info(
                        "Culling user %s (age: %s, inactive for %s)",
                        user['name'], format_td(age), format_td(inactive))
                    should_cull = True

            if not should_cull:
                app_log.debug(
                    "Not culling user %s (created: %s, last active: %s)",
                    user['name'], format_td(age), format_td(inactive))
                return False

            req = HTTPRequest(
                url=url + '/users/%s' % user['name'],
                method='DELETE',
                headers=auth_header,
            )
            yield fetch(req)
            return True

        for user in users:
            futures.append((user['name'], handle_user(user)))

        for (name, f) in futures:
            try:
                result = yield f
            except Exception:
                app_log.exception("Error processing %s", name)
            else:
                if result:
                    app_log.debug("Finished culling %s", name)


    if __name__ == '__main__':
        define(
            'url',
            default=os.environ.get('JUPYTERHUB_API_URL'),
            help="The JupyterHub API URL",
        )
        define('timeout', default=600, help="The idle timeout (in seconds)")
        define('cull_every', default=0,
               help="The interval (in seconds) for checking for idle servers to cull")
        define('max_age', default=0,
               help="The maximum age (in seconds) of servers that should be culled even if they are active")
        define('cull_users', default=False,
               help="""Cull users in addition to servers.
                    This is for use in temporary-user cases such as BinderHub.""",
               )
        define('remove_named_servers', default=False,
               help="""Remove named servers in addition to stopping them.
                    This is useful for a BinderHub that uses authentication and named servers.""",
               )
        define('concurrency', default=10,
               help="""Limit the number of concurrent requests made to the Hub.

                    Deleting a lot of users at the same time can slow down the Hub,
                    so limit the number of API requests we have outstanding at any given time.
                    """
               )

        parse_command_line()
        if not options.cull_every:
            options.cull_every = options.timeout // 2
        api_token = os.environ['JUPYTERHUB_API_TOKEN']

        try:
            AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")
        except ImportError as e:
            app_log.warning(
                "Could not load pycurl: %s\n"
                "pycurl is recommended if you have a large number of users.",
                e)

        loop = IOLoop.current()
        cull = partial(
            cull_idle,
            url=options.url,
            api_token=api_token,
            inactive_limit=options.timeout,
            cull_users=options.cull_users,
            remove_named_servers=options.remove_named_servers,
            max_age=options.max_age,
            concurrency=options.concurrency,
        )
        # schedule first cull immediately
        # because PeriodicCallback doesn't start until the end of the first interval
        loop.add_callback(cull)
        # schedule periodic cull
        pc = PeriodicCallback(cull, 1e3 * options.cull_every)
        pc.start()
        try:
            loop.start()
        except KeyboardInterrupt:
            pass
  jupyterhub_config.py: "import os\nimport re\nimport sys\n\nfrom tornado.httpclient import AsyncHTTPClient\nfrom kubernetes import client\nfrom jupyterhub.utils import url_path_join\n\n# Make sure that modules placed in the same directory as the jupyterhub config are added to the pythonpath\nconfiguration_directory = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, configuration_directory)\n\nfrom z2jh import get_config, set_config_if_not_none\n\n# Configure JupyterHub to use the curl backend for making HTTP requests,\n# rather than the pure-python implementations. The default one starts\n# being too slow to make a large number of requests to the proxy API\n# at the rate required.\nAsyncHTTPClient.configure(\"tornado.curl_httpclient.CurlAsyncHTTPClient\")\n\n# Patch for CVE-2020-15110: change default template for named servers\n# with kubespawner 0.11, {servername} *contains* a leading '-'\n# leading `{user}-{server}` to create `username--servername`,\n# preventing collision.\n# kubespawner 0.12 does not contain `-` in {servername},\n# and uses default name template `{username}--{servername}`\n# so this patch must not be used with kubespawner >= 0.12\n\nfrom distutils.version import LooseVersion as V\nfrom traitlets import default\nimport kubespawner\nfrom kubespawner import KubeSpawner\n\n\nclass PatchedKubeSpawner(KubeSpawner):\n    @default(\"pod_name_template\")\n    def _default_pod_name_template(self):\n        if self.name:\n            return \"jupyter-{username}-{servername}\"\n        else:\n            return \"jupyter-{username}\"\n\n    @default(\"pvc_name_template\")\n    def _default_pvc_name_template(self):\n        if self.name:\n            return \"claim-{username}-{servername}\"\n        else:\n            return \"claim-{username}\"\n\n\nkubespawner_version = getattr(kubespawner, \"__version__\", \"0.11\")\nif V(kubespawner_version) < V(\"0.11.999\"):\n    c.JupyterHub.spawner_class = PatchedKubeSpawner\nelse:\n    # 0.12 or greater, defaults are safe\n    c.JupyterHub.spawner_class = KubeSpawner\n\n# Connect to a proxy running in a different pod\nc.ConfigurableHTTPProxy.api_url = 'http://{}:{}'.format(os.environ['PROXY_API_SERVICE_HOST'], int(os.environ['PROXY_API_SERVICE_PORT']))\nc.ConfigurableHTTPProxy.should_start = False\n\n# Do not shut down user pods when hub is restarted\nc.JupyterHub.cleanup_servers = False\n\n# Check that the proxy has routes appropriately setup\nc.JupyterHub.last_activity_interval = 60\n\n# Don't wait at all before redirecting a spawning user to the progress page\nc.JupyterHub.tornado_settings = {\n    'slow_spawn_timeout': 0,\n}\n\n\ndef camelCaseify(s):\n    \"\"\"convert snake_case to camelCase\n\n    For the common case where some_value is set from someValue\n    so we don't have to specify the name twice.\n    \"\"\"\n    return re.sub(r\"_([a-z])\", lambda m: m.group(1).upper(), s)\n\n\n# configure the hub db connection\ndb_type = get_config('hub.db.type')\nif db_type == 'sqlite-pvc':\n    c.JupyterHub.db_url = \"sqlite:///jupyterhub.sqlite\"\nelif db_type == \"sqlite-memory\":\n    c.JupyterHub.db_url = \"sqlite://\"\nelse:\n    set_config_if_not_none(c.JupyterHub, \"db_url\", \"hub.db.url\")\n    \n\nfor trait, cfg_key in (\n    # Max number of servers that can be spawning at any one time\n    ('concurrent_spawn_limit', None),\n    # Max number of servers to be running at one time\n    ('active_server_limit', None),\n    # base url prefix\n    ('base_url', None),\n    ('allow_named_servers', None),\n    ('named_server_limit_per_user', None),\n    ('authenticate_prometheus', None),\n    ('redirect_to_server', None),\n    ('shutdown_on_logout', None),\n    ('template_paths', None),\n    ('template_vars', None),\n):\n    if cfg_key is None:\n        cfg_key = camelCaseify(trait)\n    set_config_if_not_none(c.JupyterHub, trait, 'hub.' + cfg_key)\n\nc.JupyterHub.ip = os.environ['PROXY_PUBLIC_SERVICE_HOST']\nc.JupyterHub.port = int(os.environ['PROXY_PUBLIC_SERVICE_PORT'])\n\n# the hub should listen on all interfaces, so the proxy can access it\nc.JupyterHub.hub_ip = '0.0.0.0'\n\n# implement common labels\n# this duplicates the jupyterhub.commonLabels helper\ncommon_labels = c.KubeSpawner.common_labels = {}\ncommon_labels['app'] = get_config(\n    \"nameOverride\",\n    default=get_config(\"Chart.Name\", \"jupyterhub\"),\n)\ncommon_labels['heritage'] = \"jupyterhub\"\nchart_name = get_config('Chart.Name')\nchart_version = get_config('Chart.Version')\nif chart_name and chart_version:\n    common_labels['chart'] = \"{}-{}\".format(\n        chart_name, chart_version.replace('+', '_'),\n    )\nrelease = get_config('Release.Name')\nif release:\n    common_labels['release'] = release\n\nc.KubeSpawner.namespace = os.environ.get('POD_NAMESPACE', 'default')\n\n# Max number of consecutive failures before the Hub restarts itself\n# requires jupyterhub 0.9.2\nset_config_if_not_none(\n    c.Spawner,\n    'consecutive_failure_limit',\n    'hub.consecutiveFailureLimit',\n)\n\nfor trait, cfg_key in (\n    ('start_timeout', None),\n    ('image_pull_policy', 'image.pullPolicy'),\n    ('events_enabled', 'events'),\n    ('extra_labels', None),\n    ('extra_annotations', None),\n    ('uid', None),\n    ('fs_gid', None),\n    ('service_account', 'serviceAccountName'),\n    ('storage_extra_labels', 'storage.extraLabels'),\n    ('tolerations', 'extraTolerations'),\n    ('node_selector', None),\n    ('node_affinity_required', 'extraNodeAffinity.required'),\n    ('node_affinity_preferred', 'extraNodeAffinity.preferred'),\n    ('pod_affinity_required', 'extraPodAffinity.required'),\n    ('pod_affinity_preferred', 'extraPodAffinity.preferred'),\n    ('pod_anti_affinity_required', 'extraPodAntiAffinity.required'),\n    ('pod_anti_affinity_preferred', 'extraPodAntiAffinity.preferred'),\n    ('lifecycle_hooks', None),\n    ('init_containers', None),\n    ('extra_containers', None),\n    ('mem_limit', 'memory.limit'),\n    ('mem_guarantee', 'memory.guarantee'),\n    ('cpu_limit', 'cpu.limit'),\n    ('cpu_guarantee', 'cpu.guarantee'),\n    ('extra_resource_limits', 'extraResource.limits'),\n    ('extra_resource_guarantees', 'extraResource.guarantees'),\n    ('environment', 'extraEnv'),\n    ('profile_list', None),\n    ('extra_pod_config', None),\n):\n    if cfg_key is None:\n        cfg_key = camelCaseify(trait)\n    set_config_if_not_none(c.KubeSpawner, trait, 'singleuser.' + cfg_key)\n\nimage = get_config(\"singleuser.image.name\")\nif image:\n    tag = get_config(\"singleuser.image.tag\")\n    if tag:\n        image = \"{}:{}\".format(image, tag)\n\n    c.KubeSpawner.image = image\n\nif get_config('singleuser.imagePullSecret.enabled'):\n    c.KubeSpawner.image_pull_secrets = 'singleuser-image-credentials'\n\n# scheduling:\nif get_config('scheduling.userScheduler.enabled'):\n    c.KubeSpawner.scheduler_name = os.environ['HELM_RELEASE_NAME'] + \"-user-scheduler\"\nif get_config('scheduling.podPriority.enabled'):\n    c.KubeSpawner.priority_class_name = os.environ['HELM_RELEASE_NAME'] + \"-default-priority\"\n\n# add node-purpose affinity\nmatch_node_purpose = get_config('scheduling.userPods.nodeAffinity.matchNodePurpose')\nif match_node_purpose:\n    node_selector = dict(\n        matchExpressions=[\n            dict(\n                key=\"hub.jupyter.org/node-purpose\",\n                operator=\"In\",\n                values=[\"user\"],\n            )\n        ],\n    )\n    if match_node_purpose == 'prefer':\n        c.KubeSpawner.node_affinity_preferred.append(\n            dict(\n                weight=100,\n                preference=node_selector,\n            ),\n        )\n    elif match_node_purpose == 'require':\n        c.KubeSpawner.node_affinity_required.append(node_selector)\n    elif match_node_purpose == 'ignore':\n        pass\n    else:\n        raise ValueError(\"Unrecognized value for matchNodePurpose: %r\" % match_node_purpose)\n\n# add dedicated-node toleration\nfor key in (\n    'hub.jupyter.org/dedicated',\n    # workaround GKE not supporting / in initial node taints\n    'hub.jupyter.org_dedicated',\n):\n    c.KubeSpawner.tolerations.append(\n        dict(\n            key=key,\n            operator='Equal',\n            value='user',\n            effect='NoSchedule',\n        )\n    )\n\n# Configure dynamically provisioning pvc\nstorage_type = get_config('singleuser.storage.type')\n\nif storage_type == 'dynamic':\n    pvc_name_template = get_config('singleuser.storage.dynamic.pvcNameTemplate')\n    c.KubeSpawner.pvc_name_template = pvc_name_template\n    volume_name_template = get_config('singleuser.storage.dynamic.volumeNameTemplate')\n    c.KubeSpawner.storage_pvc_ensure = True\n    set_config_if_not_none(c.KubeSpawner, 'storage_class', 'singleuser.storage.dynamic.storageClass')\n    set_config_if_not_none(c.KubeSpawner, 'storage_access_modes', 'singleuser.storage.dynamic.storageAccessModes')\n    set_config_if_not_none(c.KubeSpawner, 'storage_capacity', 'singleuser.storage.capacity')\n\n    # Add volumes to singleuser pods\n    c.KubeSpawner.volumes = [\n        {\n            'name': volume_name_template,\n            'persistentVolumeClaim': {\n                'claimName': pvc_name_template\n            }\n        }\n    ]\n    c.KubeSpawner.volume_mounts = [\n        {\n            'mountPath': get_config('singleuser.storage.homeMountPath'),\n            'name': volume_name_template\n        }\n    ]\nelif storage_type == 'static':\n    pvc_claim_name = get_config('singleuser.storage.static.pvcName')\n    c.KubeSpawner.volumes = [{\n        'name': 'home',\n        'persistentVolumeClaim': {\n            'claimName': pvc_claim_name\n        }\n    }]\n\n    c.KubeSpawner.volume_mounts = [{\n        'mountPath': get_config('singleuser.storage.homeMountPath'),\n        'name': 'home',\n        'subPath': get_config('singleuser.storage.static.subPath')\n    }]\n\nc.KubeSpawner.volumes.extend(get_config('singleuser.storage.extraVolumes', []))\nc.KubeSpawner.volume_mounts.extend(get_config('singleuser.storage.extraVolumeMounts', []))\n\n# Gives spawned containers access to the API of the hub\nc.JupyterHub.hub_connect_ip = os.environ['HUB_SERVICE_HOST']\nc.JupyterHub.hub_connect_port = int(os.environ['HUB_SERVICE_PORT'])\n\n# Allow switching authenticators easily\nauth_type = get_config('auth.type')\nemail_domain = 'local'\n\ncommon_oauth_traits = (\n        ('client_id', None),\n        ('client_secret', None),\n        ('oauth_callback_url', 'callbackUrl'),\n)\n\nif auth_type == 'google':\n    c.JupyterHub.authenticator_class = 'oauthenticator.GoogleOAuthenticator'\n    for trait, cfg_key in common_oauth_traits + (\n        ('hosted_domain', None),\n        ('login_service', None),\n    ):\n        if cfg_key is None:\n            cfg_key = camelCaseify(trait)\n        set_config_if_not_none(c.GoogleOAuthenticator, trait, 'auth.google.' + cfg_key)\n    email_domain = get_config('auth.google.hostedDomain')\nelif auth_type == 'github':\n    c.JupyterHub.authenticator_class = 'oauthenticator.github.GitHubOAuthenticator'\n    for trait, cfg_key in common_oauth_traits + (\n        ('github_organization_whitelist', 'orgWhitelist'),\n    ):\n        if cfg_key is None:\n            cfg_key = camelCaseify(trait)\n        set_config_if_not_none(c.GitHubOAuthenticator, trait, 'auth.github.' + cfg_key)\nelif auth_type == 'cilogon':\n    c.JupyterHub.authenticator_class = 'oauthenticator.CILogonOAuthenticator'\n    for trait, cfg_key in common_oauth_traits:\n        if cfg_key is None:\n            cfg_key = camelCaseify(trait)\n        set_config_if_not_none(c.CILogonOAuthenticator, trait, 'auth.cilogon.' + cfg_key)\nelif auth_type == 'gitlab':\n    c.JupyterHub.authenticator_class = 'oauthenticator.gitlab.GitLabOAuthenticator'\n    for trait, cfg_key in common_oauth_traits + (\n        ('gitlab_group_whitelist', None),\n        ('gitlab_project_id_whitelist', None),\n        ('gitlab_url', None),\n    ):\n        if cfg_key is None:\n            cfg_key = camelCaseify(trait)\n        set_config_if_not_none(c.GitLabOAuthenticator, trait, 'auth.gitlab.' + cfg_key)\nelif auth_type == 'azuread':\n    c.JupyterHub.authenticator_class = 'oauthenticator.azuread.AzureAdOAuthenticator'\n    for trait, cfg_key in common_oauth_traits + (\n        ('tenant_id', None),\n        ('username_claim', None),\n    ):\n        if cfg_key is None:\n            cfg_key = camelCaseify(trait)\n\n        set_config_if_not_none(c.AzureAdOAuthenticator, trait, 'auth.azuread.' + cfg_key)\nelif auth_type == 'mediawiki':\n    c.JupyterHub.authenticator_class = 'oauthenticator.mediawiki.MWOAuthenticator'\n    for trait, cfg_key in common_oauth_traits + (\n        ('index_url', None),\n    ):\n        if cfg_key is None:\n            cfg_key = camelCaseify(trait)\n        set_config_if_not_none(c.MWOAuthenticator, trait, 'auth.mediawiki.' + cfg_key)\nelif auth_type == 'globus':\n    c.JupyterHub.authenticator_class = 'oauthenticator.globus.GlobusOAuthenticator'\n    for trait, cfg_key in common_oauth_traits + (\n        ('identity_provider', None),\n    ):\n        if cfg_key is None:\n            cfg_key = camelCaseify(trait)\n        set_config_if_not_none(c.GlobusOAuthenticator, trait, 'auth.globus.' + cfg_key)\nelif auth_type == 'hmac':\n    c.JupyterHub.authenticator_class = 'hmacauthenticator.HMACAuthenticator'\n    c.HMACAuthenticator.secret_key = bytes.fromhex(get_config('auth.hmac.secretKey'))\nelif auth_type == 'dummy':\n    c.JupyterHub.authenticator_class = 'dummyauthenticator.DummyAuthenticator'\n    set_config_if_not_none(c.DummyAuthenticator, 'password', 'auth.dummy.password')\nelif auth_type == 'tmp':\n    c.JupyterHub.authenticator_class = 'tmpauthenticator.TmpAuthenticator'\nelif auth_type == 'lti':\n    c.JupyterHub.authenticator_class = 'ltiauthenticator.LTIAuthenticator'\n    set_config_if_not_none(c.LTIAuthenticator, 'consumers', 'auth.lti.consumers')\nelif auth_type == 'ldap':\n    c.JupyterHub.authenticator_class = 'ldapauthenticator.LDAPAuthenticator'\n    c.LDAPAuthenticator.server_address = get_config('auth.ldap.server.address')\n    set_config_if_not_none(c.LDAPAuthenticator, 'server_port', 'auth.ldap.server.port')\n    set_config_if_not_none(c.LDAPAuthenticator, 'use_ssl', 'auth.ldap.server.ssl')\n    set_config_if_not_none(c.LDAPAuthenticator, 'allowed_groups', 'auth.ldap.allowedGroups')\n    set_config_if_not_none(c.LDAPAuthenticator, 'bind_dn_template', 'auth.ldap.dn.templates')\n    set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn', 'auth.ldap.dn.lookup')\n    set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_search_filter', 'auth.ldap.dn.search.filter')\n    set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_search_user', 'auth.ldap.dn.search.user')\n    set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_search_password', 'auth.ldap.dn.search.password')\n    set_config_if_not_none(c.LDAPAuthenticator, 'lookup_dn_user_dn_attribute', 'auth.ldap.dn.user.dnAttribute')\n    set_config_if_not_none(c.LDAPAuthenticator, 'escape_userdn', 'auth.ldap.dn.user.escape')\n    set_config_if_not_none(c.LDAPAuthenticator, 'valid_username_regex', 'auth.ldap.dn.user.validRegex')\n    set_config_if_not_none(c.LDAPAuthenticator, 'user_search_base', 'auth.ldap.dn.user.searchBase')\n    set_config_if_not_none(c.LDAPAuthenticator, 'user_attribute', 'auth.ldap.dn.user.attribute')\nelif auth_type == 'custom':\n    # full_class_name looks like \"myauthenticator.MyAuthenticator\".\n    # To create a docker image with this class availabe, you can just have the\n    # following Dockerfile:\n    #   FROM jupyterhub/k8s-hub:v0.4\n    #   RUN pip3 install myauthenticator\n    full_class_name = get_config('auth.custom.className')\n    c.JupyterHub.authenticator_class = full_class_name\n    auth_class_name = full_class_name.rsplit('.', 1)[-1]\n    auth_config = c[auth_class_name]\n    auth_config.update(get_config('auth.custom.config') or {})\nelse:\n    raise ValueError(\"Unhandled auth type: %r\" % auth_type)\n\nset_config_if_not_none(c.OAuthenticator, 'scope', 'auth.scopes')\n\nset_config_if_not_none(c.Authenticator, 'enable_auth_state', 'auth.state.enabled')\n\n# Enable admins to access user servers\nset_config_if_not_none(c.JupyterHub, 'admin_access', 'auth.admin.access')\nset_config_if_not_none(c.Authenticator, 'admin_users', 'auth.admin.users')\nset_config_if_not_none(c.Authenticator, 'whitelist', 'auth.whitelist.users')\n\nc.JupyterHub.services = []\n\nif get_config('cull.enabled', False):\n    cull_cmd = [\n        'python3',\n        '/etc/jupyterhub/cull_idle_servers.py',\n    ]\n    base_url = c.JupyterHub.get('base_url', '/')\n    cull_cmd.append(\n        '--url=http://127.0.0.1:8081' + url_path_join(base_url, 'hub/api')\n    )\n\n    cull_timeout = get_config('cull.timeout')\n    if cull_timeout:\n        cull_cmd.append('--timeout=%s' % cull_timeout)\n\n    cull_every = get_config('cull.every')\n    if cull_every:\n        cull_cmd.append('--cull-every=%s' % cull_every)\n\n    cull_concurrency = get_config('cull.concurrency')\n    if cull_concurrency:\n        cull_cmd.append('--concurrency=%s' % cull_concurrency)\n\n    if get_config('cull.users'):\n        cull_cmd.append('--cull-users')\n\n    if get_config('cull.removeNamedServers'):\n        cull_cmd.append('--remove-named-servers')\n\n    cull_max_age = get_config('cull.maxAge')\n    if cull_max_age:\n        cull_cmd.append('--max-age=%s' % cull_max_age)\n\n    c.JupyterHub.services.append({\n        'name': 'cull-idle',\n        'admin': True,\n        'command': cull_cmd,\n    })\n\nfor name, service in get_config('hub.services', {}).items():\n    # jupyterhub.services is a list of dicts, but\n    # in the helm chart it is a dict of dicts for easier merged-config\n    service.setdefault('name', name)\n    # handle camelCase->snake_case of api_token\n    api_token = service.pop('apiToken', None)\n    if api_token:\n        service['api_token'] = api_token\n    c.JupyterHub.services.append(service)\n\n\nset_config_if_not_none(c.Spawner, 'cmd', 'singleuser.cmd')\nset_config_if_not_none(c.Spawner, 'default_url', 'singleuser.defaultUrl')\n\ncloud_metadata = get_config('singleuser.cloudMetadata', {})\n\nif not cloud_metadata.get('enabled', False):\n    # Use iptables to block access to cloud metadata by default\n    network_tools_image_name = get_config('singleuser.networkTools.image.name')\n    network_tools_image_tag = get_config('singleuser.networkTools.image.tag')\n    ip_block_container = client.V1Container(\n        name=\"block-cloud-metadata\",\n        image=f\"{network_tools_image_name}:{network_tools_image_tag}\",\n        command=[\n            'iptables',\n            '-A', 'OUTPUT',\n            '-d', cloud_metadata.get('ip', '169.254.169.254'),\n            '-j', 'DROP'\n        ],\n        security_context=client.V1SecurityContext(\n            privileged=True,\n            run_as_user=0,\n            capabilities=client.V1Capabilities(add=['NET_ADMIN'])\n        )\n    )\n\n    c.KubeSpawner.init_containers.append(ip_block_container)\n\n\nif get_config('debug.enabled', False):\n    c.JupyterHub.log_level = 'DEBUG'\n    c.Spawner.debug = True\n\n\nextra_config = get_config('hub.extraConfig', {})\nif isinstance(extra_config, str):\n    from textwrap import indent, dedent\n    msg = dedent(\n    \"\"\"\n    hub.extraConfig should be a dict of strings,\n    but found a single string instead.\n\n    extraConfig as a single string is deprecated\n    as of the jupyterhub chart version 0.6.\n\n    The keys can be anything identifying the\n    block of extra configuration.\n\n    Try this instead:\n\n        hub:\n          extraConfig:\n            myConfig: |\n              {}\n\n    This configuration will still be loaded,\n    but you are encouraged to adopt the nested form\n    which enables easier merging of multiple extra configurations.\n    \"\"\"\n    )\n    print(\n        msg.format(\n            indent(extra_config, ' ' * 10).lstrip()\n        ),\n        file=sys.stderr\n    )\n    extra_config = {'deprecated string': extra_config}\n\nfor key, config_py in sorted(extra_config.items()):\n    print(\"Loading extra config: %s\" % key)\n    exec(config_py)\n"
  z2jh.py: |
    """
    Utility methods for use in jupyterhub_config.py and dynamic subconfigs.

    Methods here can be imported by extraConfig in values.yaml
    """
    from collections import Mapping
    from functools import lru_cache
    import os

    import yaml


    # memoize so we only load config once
    @lru_cache()
    def _load_config():
        """Load configuration from disk

        Memoized to only load once
        """
        cfg = {}
        for source in ('config', 'secret'):
            path = f"/etc/jupyterhub/{source}/values.yaml"
            if os.path.exists(path):
                print(f"Loading {path}")
                with open(path) as f:
                    values = yaml.safe_load(f)
                cfg = _merge_dictionaries(cfg, values)
            else:
                print(f"No config at {path}")
        return cfg


    def _merge_dictionaries(a, b):
        """Merge two dictionaries recursively.

        Simplified From https://stackoverflow.com/a/7205107
        """
        merged = a.copy()
        for key in b:
            if key in a:
                if isinstance(a[key], Mapping) and isinstance(b[key], Mapping):
                    merged[key] = _merge_dictionaries(a[key], b[key])
                else:
                    merged[key] = b[key]
            else:
                merged[key] = b[key]
        return merged


    def get_config(key, default=None):
        """
        Find a config item of a given name & return it

        Parses everything as YAML, so lists and dicts are available too

        get_config("a.b.c") returns config['a']['b']['c']
        """
        value = _load_config()
        # resolve path in yaml
        for level in key.split('.'):
            if not isinstance(value, dict):
                # a parent is a scalar or null,
                # can't resolve full path
                return default
            if level not in value:
                return default
            else:
                value = value[level]
        return value


    def set_config_if_not_none(cparent, name, key):
        """
        Find a config item of a given name, set the corresponding Jupyter
        configuration item if not None
        """
        data = get_config(key)
        if data is not None:
            setattr(cparent, name, data)
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
data:
  policy.cfg: '{"alwaysCheckAllPredicates":false,"apiVersion":"v1","hardPodAffinitySymmetricWeight":100,"kind":"Policy","predicates":[{"name":"PodFitsResources"},{"name":"HostName"},{"name":"PodFitsHostPorts"},{"name":"MatchNodeSelector"},{"name":"NoDiskConflict"},{"name":"PodToleratesNodeTaints"},{"name":"MaxEBSVolumeCount"},{"name":"MaxGCEPDVolumeCount"},{"name":"MaxAzureDiskVolumeCount"},{"name":"CheckVolumeBinding"},{"name":"NoVolumeZoneConflict"},{"name":"MatchInterPodAffinity"}],"priorities":[{"name":"NodePreferAvoidPodsPriority","weight":161051},{"name":"NodeAffinityPriority","weight":14641},{"name":"InterPodAffinityPriority","weight":1331},{"name":"MostRequestedPriority","weight":121},{"name":"ImageLocalityPriority","weight":11}]}'
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: hub-db-dir
  labels:
    component: hub
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-user-scheduler-complementary
  labels:
    component: user-scheduler-complementary
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
rules:
  - apiGroups:
      - ""
    resourceNames:
      - user-scheduler
    resources:
      - configmaps
    verbs:
      - get
      - update
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - persistentvolume
      - persistentvolumeclaims
    verbs:
      - update
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-user-scheduler-base
  labels:
    component: user-scheduler-base
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: user-scheduler
    namespace: default
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-user-scheduler-complementary
  labels:
    component: user-scheduler-complementary
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: user-scheduler
    namespace: default
roleRef:
  kind: ClusterRole
  name: release-name-user-scheduler-complementary
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - persistentvolumeclaims
    verbs:
      - get
      - watch
      - list
      - create
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - watch
      - list
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: hub
    namespace: default
roleRef:
  kind: Role
  name: hub
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: Service
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: /hub/metrics
spec:
  type: ClusterIP
  selector:
    component: hub
    app: jupyterhub
    release: release-name
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
---
apiVersion: v1
kind: Service
metadata:
  name: proxy-api
  labels:
    component: proxy-api
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  selector:
    component: proxy
    app: jupyterhub
    release: release-name
  ports:
    - protocol: TCP
      port: 8001
      targetPort: 8001
---
apiVersion: v1
kind: Service
metadata:
  name: proxy-public
  labels:
    component: proxy-public
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  selector:
    component: proxy
    release: release-name
  ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 443
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8000
  type: LoadBalancer
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: continuous-image-puller
  labels:
    component: continuous-image-puller
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  selector:
    matchLabels:
      component: continuous-image-puller
      app: jupyterhub
      release: release-name
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 100%
  template:
    metadata:
      labels:
        component: continuous-image-puller
        app: jupyterhub
        release: release-name
    spec:
      tolerations:
        - key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
          effect: NoSchedule
        - key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
          effect: NoSchedule
      nodeSelector: {}
      terminationGracePeriodSeconds: 0
      automountServiceAccountToken: false
      initContainers:
        - name: image-pull-singleuser
          image: biodatageeks/pysequila-notebook:0.1.3-ga5af501
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
        - name: image-pull-metadata-block
          image: jupyterhub/k8s-network-tools:0.9.1
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
      containers:
        - name: pause
          image: gcr.io/google_containers/pause:3.1
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: hub
      app: jupyterhub
      release: release-name
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        component: hub
        app: jupyterhub
        release: release-name
        hub.jupyter.org/network-access-proxy-api: "true"
        hub.jupyter.org/network-access-proxy-http: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        checksum/config-map: 065ab732e50f600835fe1a4ba6d6facc1f34b88bb7edf85c73aecc1bcbf7351f
        checksum/secret: 1094d68d7062379e03742840364280a1cc2fa331d3437dc42ebcff0cee60b00c
    spec:
      nodeSelector: {}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values:
                      - core
      volumes:
        - name: config
          configMap:
            name: hub-config
        - name: secret
          secret:
            secretName: hub-secret
        - name: hub-db-dir
          persistentVolumeClaim:
            claimName: hub-db-dir
      serviceAccountName: hub
      securityContext:
        fsGroup: 1000
      containers:
        - name: hub
          image: jupyterhub/k8s-hub:0.9.1
          command:
            - jupyterhub
            - --config
            - /etc/jupyterhub/jupyterhub_config.py
            - --upgrade-db
          volumeMounts:
            - mountPath: /etc/jupyterhub/jupyterhub_config.py
              subPath: jupyterhub_config.py
              name: config
            - mountPath: /etc/jupyterhub/z2jh.py
              subPath: z2jh.py
              name: config
            - mountPath: /etc/jupyterhub/cull_idle_servers.py
              subPath: cull_idle_servers.py
              name: config
            - mountPath: /etc/jupyterhub/config/
              name: config
            - mountPath: /etc/jupyterhub/secret/
              name: secret
            - mountPath: /srv/jupyterhub
              name: hub-db-dir
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            runAsUser: 11146
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          env:
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: HELM_RELEASE_NAME
              value: release-name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hub-secret
                  key: proxy.token
          ports:
            - containerPort: 8081
              name: hub
          readinessProbe:
            initialDelaySeconds: 0
            periodSeconds: 10
            httpGet:
              path: /hub/health
              port: hub
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: proxy
  labels:
    component: proxy
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: proxy
      app: jupyterhub
      release: release-name
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        component: proxy
        app: jupyterhub
        release: release-name
        hub.jupyter.org/network-access-hub: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        checksum/hub-secret: ea1d4268fe8d6feef50b1867177e1ffed7d2706007c04af685a0902b78b04655
        checksum/proxy-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      terminationGracePeriodSeconds: 60
      nodeSelector: {}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values:
                      - core
      containers:
        - name: chp
          image: jupyterhub/configurable-http-proxy:4.2.1
          command:
            - configurable-http-proxy
            - --ip=0.0.0.0
            - --api-ip=0.0.0.0
            - --api-port=8001
            - --default-target=http://$(HUB_SERVICE_HOST):$(HUB_SERVICE_PORT)
            - --error-target=http://$(HUB_SERVICE_HOST):$(HUB_SERVICE_PORT)/hub/error
            - --port=8000
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                "": NET_RAW
            readOnlyRootFilesystem: true
          env:
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hub-secret
                  key: proxy.token
          ports:
            - containerPort: 8000
              name: proxy-public
            - containerPort: 8001
              name: api
          livenessProbe:
            initialDelaySeconds: 30
            periodSeconds: 10
            httpGet:
              path: /_chp_healthz
              port: proxy-public
              scheme: HTTP
          readinessProbe:
            initialDelaySeconds: 0
            periodSeconds: 10
            httpGet:
              path: /_chp_healthz
              port: proxy-public
              scheme: HTTP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  replicas: 2
  selector:
    matchLabels:
      component: user-scheduler
      app: jupyterhub
      release: release-name
  template:
    metadata:
      labels:
        component: user-scheduler
        app: jupyterhub
        release: release-name
      annotations:
        checksum/config-map: 1faff3ba83d005ff41f9d1ac430671413df0bb1cdda4e6bda9099609f6bb5eda
    spec:
      serviceAccountName: user-scheduler
      nodeSelector: {}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values:
                      - core
      containers:
        - name: user-scheduler
          image: gcr.io/google_containers/kube-scheduler-amd64:v1.13.12
          command:
            - /usr/local/bin/kube-scheduler
            - --scheduler-name=release-name-user-scheduler
            - --policy-configmap=user-scheduler
            - --policy-configmap-namespace=default
            - --lock-object-name=user-scheduler
            - --lock-object-namespace=default
            - --leader-elect-resource-lock=configmaps
            - --v=4
          livenessProbe:
            httpGet:
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
          readinessProbe:
            httpGet:
              path: /healthz
              port: 10251
          resources:
            requests:
              cpu: 50m
              memory: 256Mi
            seccompProfile:
              type: RuntimeDefault
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: user-placeholder
  labels:
    component: user-placeholder
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
spec:
  podManagementPolicy: Parallel
  replicas: 0
  selector:
    matchLabels:
      component: user-placeholder
      app: jupyterhub
      release: release-name
  serviceName: user-placeholder
  template:
    metadata:
      labels:
        component: user-placeholder
        app: jupyterhub
        release: release-name
    spec:
      schedulerName: release-name-user-scheduler
      tolerations:
        - key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
          effect: NoSchedule
        - key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
          effect: NoSchedule
      nodeSelector: {}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values:
                      - user
      terminationGracePeriodSeconds: 0
      automountServiceAccountToken: false
      containers:
        - name: pause
          image: gcr.io/google_containers/pause:3.1
          resources:
            requests:
              memory: 1G
            seccompProfile:
              type: RuntimeDefault
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "0"
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "0"
rules:
  - apiGroups:
      - apps
    resources:
      - daemonsets
    verbs:
      - get
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "0"
subjects:
  - kind: ServiceAccount
    name: hook-image-awaiter
    namespace: default
roleRef:
  kind: Role
  name: hook-image-awaiter
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: hook-image-puller
  labels:
    component: hook-image-puller
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "-10"
spec:
  selector:
    matchLabels:
      component: hook-image-puller
      app: jupyterhub
      release: release-name
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 100%
  template:
    metadata:
      labels:
        component: hook-image-puller
        app: jupyterhub
        release: release-name
    spec:
      tolerations:
        - key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
          effect: NoSchedule
        - key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
          effect: NoSchedule
      nodeSelector: {}
      terminationGracePeriodSeconds: 0
      automountServiceAccountToken: false
      initContainers:
        - name: image-pull-singleuser
          image: biodatageeks/pysequila-notebook:0.1.3-ga5af501
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
        - name: image-pull-metadata-block
          image: jupyterhub/k8s-network-tools:0.9.1
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
      containers:
        - name: pause
          image: gcr.io/google_containers/pause:3.1
---
apiVersion: batch/v1
kind: Job
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: release-name
    chart: jupyterhub-0.9.1
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "10"
spec:
  template:
    metadata:
      labels:
        component: image-puller
        app: jupyterhub
        release: release-name
    spec:
      restartPolicy: Never
      serviceAccountName: hook-image-awaiter
      containers:
        - image: jupyterhub/k8s-image-awaiter:0.9.1
          name: hook-image-awaiter
          imagePullPolicy: IfNotPresent
          command:
            - /image-awaiter
            - -ca-path=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            - -auth-token-path=/var/run/secrets/kubernetes.io/serviceaccount/token
            - -api-server-address=https://$(KUBERNETES_SERVICE_HOST):$(KUBERNETES_SERVICE_PORT)
            - -namespace=default
            - -daemonset=hook-image-puller
