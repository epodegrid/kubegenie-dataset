apiVersion: v1
kind: Secret
metadata:
  name: release-na-ibm-db2warehouse-db2u-ldap
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
type: Opaque
data:
  password: V2JUWmVuekQ2MmZvZllwczZ1WXRYdVBwVA==
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-na-ibm-db2warehouse-db2u-config
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    run: release-na-ibm-db2warehouse-db2u-config
data:
  db2u-var: |
    SERVICE_NAME ""
    RUNTIME_ENV LOCAL
    SSH_PORT 50022
    BLUADMPASS ""
    BLUDR_ENABLED NO
    BLUDR_EXTMNT_CFG_FILE ""
    REBALANCE_TABLESPACE NO
    SSL_CERT_FILE ""
    SSL_CERT_KEY_FILE ""
    SSL_CERT_DB_TYPE p12
    OBJECT_STORE_ENDPOINT ""
    STAGING_PATH /local/db2inst1/staging
    WV_HACLASS UDB
    FCM_IPS ""
    DB2TYPE "db2wh"
    DBNAME "BLUDB"
    DBWORKLOAD ""
    MLN_TOTAL "1"
    MLN_DISTRIBUTION "0:0"
    DB_PATH /mnt/blumeta0/db2/databases
    TABLE_ORG "COLUMN"
    DB_PAGE_SIZE "32768"
    ENCRYPT_DB "YES"
    INSTANCE_MEMORY_PERCENT "80"
    DB2_COMPATIBILITY_VECTOR "NULL"
    DB_CODESET "UTF-8"
    DB_TERRITORY "US"
    DB_COLLATION_SEQUENCE "IDENTITY"
    DB2INSTANCE "db2inst1"
    INST_GROUP "db2iadm1"
    FENCED_USER "db2fenc1"
    DB_USER_UID 500
    DB_ADM_GROUP_GID 1003
    DB_FENCED_USER_UID 501
    DB_FENCED_GROUP_GID 1002
    DBM_CFG_FILE /mnt/blumeta0/db2_config/custom_dbm.cfg
    DB_CFG_FILE /mnt/blumeta0/db2_config/custom_db.cfg
    DB2_REGVAR_FILE /mnt/blumeta0/db2_config/custom_registry.cfg
    LDAP_ENABLED true
    LDAP_SERVER ""
    LDAP_DOMAIN "blustratus"
    LDAP_ADMIN "bluldap"
    LDAP_USER_GROUP "bluusers"
    LDAP_ADMIN_GROUP "bluadmin"
    DB2_4K_DEV_SUPPORT "false"
    LDAP_PORT 50389
  db2u-future-var: |
    EXTERNAL_SSH_ENABLED NO
    DISABLE_SPARK YES
    SPARK_MEMORY_SHARE 10
    TIMEZONE UTC
    ENABLE_ANALYTICS_ACCELERATOR NO
    STIG_HARDENING NO
    DB2W_UNIFIED_CONSOLE NO
    KUBERSMP NO
    IAM_ENABLED NO
    IAM_STASH_FILE ""
    DATA_ON_MLN0 NO
    SEGMENT_CRED_FILE ""
    ICP4D NO
    NAMESPACE ""
    DNS_SUBDOMAIN ""
    ASPERA_HOST ""
    GUARDIUM_INFO ""
    PRUNE_LOGS_SCHEDULE 2
    USAGE_TRACKING_ENABLED NO
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-na-ibm-db2warehouse-db2u-uc-config
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
data:
  METADB_USER: db2inst1
  METADB_HOST: release-na-ibm-db2warehouse-db2u-engn-svc
  METADB_PORT: "50000"
  METADB_SSL: "false"
  METADB_SECURITY_MECHANISM: "3"
  METADB_DB_NAME: BLUDB
  METADB_DS_EXT_TYPE: DB2LUW
  METADB_DB2IADM1_GROUP_ID: "1000"
  INSTANCE_ID: ""
  DISABLE_HOST_CHECK: "true"
  SHARED_PV_MOUNTED: "true"
  POD_NAMESPACE: default
  DSSERVER_USER_HOME: /mnt/blumeta0/home
  LDAP_HOST: release-na-ibm-db2warehouse-db2u-ldap
  LDAP_PORT: "50389"
  LDAP_BASE_DN: dc=blustratus,dc=com
  LDAP_ROOT_DN: cn=bluldap,dc=blustratus,dc=com
  LDAP_USER_GROUP: bluusers
  LDAP_ADMIN_GROUP: bluadmin
  LDAP_SSL_METHOD: starttls
  CREATE_CUSTOMER_DB_PROFILE: "true"
  PLATFORM_CODE: PLATFORM_K8S_db2wh_smp
  CP_NAMESPACE: default
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-na-ibm-db2warehouse-db2u-hadr-config
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    run: release-na-ibm-db2warehouse-db2u-hadr-config
data:
  db2u-hadr-var: |
    HADR_ENABLED "false"
    HADR_INIT_ROLE "STANDARD"
    HADR_PRIMARY_PORT "60006"
    HADR_STANDBY_PORT "60007"
    HADR_REMOTE_HOST "NULL"
    HADR_TARGET_LIST "NULL"
    HADR_REMOTE_INST "db2inst1"
    HADR_TIMEOUT "120"
    HADR_SYNCMODE "NEARSYNC"
    HADR_PEER_WINDOW "120"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-na-ibm-db2warehouse-db2u-lic
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    run: release-na-ibm-db2warehouse-db2u-lic
data:
  db2u-lic: |
    [LicenseCertificate]
    CheckSum=C37CDFAE6E5CE335500B4876C6E73444
    TimeStamp=1496664869
    PasswordVersion=4
    VendorName=IBM Toronto Lab
    VendorPassword=7v8p4fq2dtfpc
    VendorID=5fbee0ee6feb.02.09.15.0f.48.00.00.00
    ProductName=dashDB
    ProductID=2057
    ProductVersion=1.0.0
    ProductPassword=5cn2tur9wj7hnhk4agessatp
    ProductAnnotation=6; (_uw)
    LicenseStyle=nodelocked
    LicenseStartDate=06/05/2017
    LicenseDuration=7515
    LicenseEndDate=12/31/2037
    LicenseCount=1
    MultiUseRules=
    RegistrationLevel=3
    TryAndBuy=No
    SoftStop=No
    TargetType=ANY
    TargetTypeName=Open Target
    TargetID=ANY
    ExtendedTargetType=
    ExtendedTargetID=
    DerivedLicenseStyle=
    DerivedLicenseStartDate=
    DerivedLicenseEndDate=
    DerivedLicenseAggregateDuration=
    SerialNumber=
    Upgrade=No
    CapacityType=
    Bundle=No
    InstallProgram=
    AdditionalLicenseData=
    CustomAttribute1=No
    CustomAttribute2=No
    CustomAttribute3=No
    SubCapacityEligibleProduct=No
    MaxOfflinePeriod=
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-na-ibm-db2warehouse-db2u-wv-config
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    run: release-na-ibm-db2warehouse-db2u-wv-config
data:
  db2u-wolverine-var: |
    disabled_dms: '[]'
    ha_enabled: "true"
    settings.cluster_quorum_timeout: "300"
    settings.config_file: '{metadir}/wolverine/config.json'
    settings.dm_status_ttl: "30"
    settings.etcd_client_port: "2379"
    settings.etcd_data_dir: '{metadir}/wolverine/{pod}/etcd'
    settings.etcd_server_port: "2380"
    settings.ha_loop_frequency: "10"
    settings.heartbeat_timeout: "30"
    settings.leader_election_timeout: "90"
    settings.local_filesystems: '{"scratch": "/scratch", "data": "/data", "local": "/local"}'
    settings.log_file_path: '{metadir}/wolverine/{pod}/logs/ha.log'
    settings.node_failover_timeout: "30"
    settings.rest-server-port: "8443"
    settings.shared_filesystems: '{"head": "/head", "ldaphome": "/ldaphome"}'
    settings.startup_cluster_quorum_timeout: "120"
    settings.startup_heartbeat_timeout: "120"
    settings.startup_leader_timeout: "120"
    settings.startup_recovery_ready_timeout: "30"
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: release-na-ibm-db2warehouse-db2u-sqllib-shared
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
spec:
  storageClassName: ""
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 40Gi
---
apiVersion: v1
kind: Service
metadata:
  name: release-na-ibm-db2warehouse-db2u-engn-svc
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    component: db2wh
    release: release-name
    heritage: Helm
spec:
  type: NodePort
  ports:
    - port: 50000
      targetPort: 50000
      protocol: TCP
      name: legacy-server
    - port: 50001
      targetPort: 50001
      protocol: TCP
      name: ssl-server
  selector:
    app: release-na-ibm-db2warehouse
    component: db2wh
    type: engine
---
apiVersion: v1
kind: Service
metadata:
  name: release-na-ibm-db2warehouse-etcd
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    component: etcd
    release: release-name
    heritage: Helm
    icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
    icpdsupport/app: release-na-ibm-db2warehouse
spec:
  ports:
    - port: 2380
      name: etcd-server
    - port: 2379
      name: etcd-client
      protocol: TCP
  clusterIP: None
  selector:
    app: release-na-ibm-db2warehouse
    component: etcd
---
apiVersion: v1
kind: Service
metadata:
  name: release-na-ibm-db2warehouse-db2u-ldap
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    component: ldap
    release: release-name
    heritage: Helm
spec:
  ports:
    - port: 50389
      targetPort: 50389
      protocol: TCP
      name: release-na-ibm-db2warehouse-db2u-ldap
  selector:
    app: release-na-ibm-db2warehouse
    component: ldap
---
apiVersion: v1
kind: Service
metadata:
  name: release-na-ibm-db2warehouse-db2u-rest-svc
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    component: db2wh
    release: release-name
    heritage: Helm
spec:
  type: NodePort
  ports:
    - port: 50050
      targetPort: 50050
      protocol: TCP
      name: rest-server
  selector:
    app: release-na-ibm-db2warehouse
    component: db2wh
    type: rest
---
apiVersion: v1
kind: Service
metadata:
  name: release-na-ibm-db2warehouse-db2u
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    component: db2wh
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 50000
      targetPort: 50000
      protocol: TCP
      name: db2-server
    - port: 50001
      targetPort: 50001
      protocol: TCP
      name: db2-ssl-server
    - port: 25000
      targetPort: 25000
      protocol: TCP
      name: spark-p25000
    - port: 25001
      targetPort: 25001
      protocol: TCP
      name: spark-p25001
    - port: 25002
      targetPort: 25002
      protocol: TCP
      name: spark-p25002
    - port: 25003
      targetPort: 25003
      protocol: TCP
      name: spark-p25003
    - port: 25004
      targetPort: 25004
      protocol: TCP
      name: spark-p25004
    - port: 25005
      targetPort: 25005
      protocol: TCP
      name: spark-p25005
  selector:
    app: release-na-ibm-db2warehouse
    component: db2wh
    type: engine
---
apiVersion: v1
kind: Service
metadata:
  name: release-na-ibm-db2warehouse-db2u-internal
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    component: db2wh
    type: engine
    release: release-name
    heritage: Helm
spec:
  ports:
    - port: 50000
      name: main
      targetPort: 50000
      protocol: TCP
    - port: 9443
      name: wvha-rest
      targetPort: 9443
      protocol: TCP
  clusterIP: None
  selector:
    app: release-na-ibm-db2warehouse
    component: db2wh
    type: engine
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-na-ibm-db2warehouse-db2u-ldap
  labels:
    app: release-na-ibm-db2warehouse
    release: release-name
    component: ldap
    heritage: Helm
    chart: ibm-db2warehouse
    icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
    icpdsupport/app: release-na-ibm-db2warehouse
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: release-na-ibm-db2warehouse
      chart: ibm-db2warehouse
      release: release-name
      component: ldap
      heritage: Helm
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: ibm-db2warehouse
        release: release-name
        component: ldap
        heritage: Helm
        icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:
        productName: Db2 Warehouse
        productID: Community
        productVersion: 11.5.4.0
    spec:
      volumes:
        - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
          persistentVolumeClaim:
            claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - name: ldap-secret
          secret:
            secretName: release-na-ibm-db2warehouse-db2u-ldap
            defaultMode: 292
        - name: bluadmin-secret
          secret:
            secretName: release-na-ibm-db2warehouse-db2u-ldap-bluadmin
            defaultMode: 292
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      serviceAccount: db2u
      initContainers:
        - name: init-ldapdb-dir
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          command:
            - /bin/sh
            - -ecx
            - |
              /tools/post-install/db2u_ready.sh \
                  --replicas 1 \
                  --template release-na-ibm-db2warehouse \
                  --namespace default \
                  --dbType db2wh
              DETERMINATION_FILE=/mnt/blumeta0/nodeslist
              CAT_NODE=$(head -1 $DETERMINATION_FILE)
              kubectl exec -it -n default ${CAT_NODE?} -- bash -c \
                  "sudo mkdir -p /mnt/blumeta0/ldap_db /mnt/blumeta0/ldap_rootCA \
                  && sudo chown -c -R 55:55 /mnt/blumeta0/ldap_db /mnt/blumeta0/ldap_rootCA \
                  && sudo chmod -c -R 640 /mnt/blumeta0/ldap_db \
                  && sudo chmod -c 700 /mnt/blumeta0/ldap_db"
              exit $?
          volumeMounts:
            - mountPath: /mnt/blumeta0
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
      containers:
        - name: ldap
          image: icr.io/obs/hdm/db2u/db2u.auxiliary.auth:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 55
            capabilities:
              add:
                - CHOWN
                - NET_BIND_SERVICE
                - DAC_OVERRIDE
                - SETGID
                - SETUID
                - KILL
              drop:
                - ALL
          readinessProbe:
            timeoutSeconds: 3
            initialDelaySeconds: 10
            exec:
              command:
                - /bin/sh
                - -c
                - test -f /opt/ibm/scripts/.ldap_initialized
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 5
          livenessProbe:
            timeoutSeconds: 10
            initialDelaySeconds: 20
            exec:
              command:
                - /bin/sh
                - -c
                - test -f /opt/ibm/scripts/.ldap_initialized
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 5
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
            limits:
              cpu: 200m
              memory: 2Gi
          env:
            - name: INSTANCE_NAME
              value: db2inst1
            - name: SERVICE_NAME
              value: release-na-ibm-db2warehouse-db2u-ldap
            - name: LDAP_PWD
              valueFrom:
                secretKeyRef:
                  name: release-na-ibm-db2warehouse-db2u-ldap
                  key: password
          volumeMounts:
            - mountPath: /mnt/blumeta0
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
            - name: ldap-secret
              mountPath: /secrets/bluldap.pwd
              readOnly: true
            - name: bluadmin-secret
              mountPath: /secrets/bluadmin.pwd
              readOnly: true
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-na-ibm-db2warehouse-db2u-tools
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    component: db2wh
    icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
    icpdsupport/app: release-na-ibm-db2warehouse
spec:
  replicas: 1
  selector:
    matchLabels:
      type: tools
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: ibm-db2warehouse
        release: release-name
        heritage: Helm
        component: db2wh
        api-database-status: db2wh-api
        type: tools
        icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:
        productName: Db2 Warehouse
        productID: Community
        productVersion: 11.5.4.0
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      securityContext:
        runAsNonRoot: true
      serviceAccount: db2u
      containers:
        - name: release-na-ibm-db2warehouse-db2u-tools
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          env:
            - name: SERVICE_NAME
              value: release-na-ibm-db2warehouse
            - name: DB2U_SERVICE_NAME
              value: release-na-ibm-db2warehouse-db2u
          volumeMounts:
            - mountPath: /mnt/blumeta0
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
            - mountPath: /mnt/blumeta0/configmap/hadr
              name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
            - mountPath: /secrets/db2instancepwd
              name: db2instance-secret
              readOnly: true
            - name: ldap-secret
              mountPath: /secrets/bluldap.pwd
              readOnly: true
      volumes:
        - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
          persistentVolumeClaim:
            claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
          configMap:
            name: release-na-ibm-db2warehouse-db2u-hadr-config
        - name: db2instance-secret
          secret:
            secretName: release-na-ibm-db2warehouse-db2u-instance
            defaultMode: 256
        - name: ldap-secret
          secret:
            secretName: release-na-ibm-db2warehouse-db2u-ldap
            defaultMode: 292
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-na-ibm-db2warehouse-etcd
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    component: etcd
    release: release-name
    heritage: Helm
    icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
    icpdsupport/app: release-na-ibm-db2warehouse
spec:
  selector:
    matchLabels:
      app: release-na-ibm-db2warehouse
      release: release-name
      heritage: Helm
      component: etcd
  serviceName: release-na-ibm-db2warehouse-etcd
  replicas: 3
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: ibm-db2warehouse
        component: etcd
        release: release-name
        heritage: Helm
        icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:
        productName: Db2 Warehouse
        productID: Community
        productVersion: 11.5.4.0
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      serviceAccount: db2u
      terminationGracePeriodSeconds: 0
      volumes:
        - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
          persistentVolumeClaim:
            claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
      initContainers:
        - name: init-etcd-dir
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          command:
            - /bin/sh
            - -ec
            - |
              set -x
              /tools/post-install/db2u_ready.sh \
                  --replicas 1 \
                  --template release-na-ibm-db2warehouse \
                  --namespace default \
                  --dbType db2wh
              DETERMINATION_FILE=/persistence/nodeslist
              CAT_NODE=$(head -1 $DETERMINATION_FILE)
              kubectl exec -it -n default ${CAT_NODE?} -- bash -c \
                  "sudo mkdir -p /mnt/blumeta0/etcd \
                  && sudo chmod -c -R 777 /mnt/blumeta0/etcd"
              exit $?
          volumeMounts:
            - mountPath: /persistence
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
      containers:
        - name: release-na-ibm-db2warehouse-etcd
          image: icr.io/obs/hdm/db2u/etcd:3.3.10-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
            capabilities:
              drop:
                - ALL
          livenessProbe:
            tcpSocket:
              port: 2379
            timeoutSeconds: 10
            initialDelaySeconds: 20
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 5
          ports:
            - containerPort: 2380
              name: peer
            - containerPort: 2379
              name: client
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
          env:
            - name: INITIAL_CLUSTER_SIZE
              value: "3"
            - name: SET_NAME
              value: release-na-ibm-db2warehouse-etcd
          volumeMounts:
            - mountPath: /persistence
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/sh
                  - -ec
                  - |
                    EPS=""
                    for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                        EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
                    done
                    HOSTNAME=$(hostname)
                    member_hash() {
                        etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1
                    }
                    SET_ID=${HOSTNAME##*[^0-9]}
                    if [ "${SET_ID}" -ge ${INITIAL_CLUSTER_SIZE} ]; then
                        echo "Removing ${HOSTNAME} from etcd cluster"
                        ETCDCTL_ENDPOINT=${EPS} etcdctl member remove $(member_hash)
                        if [ $? -eq 0 ]; then
                            # Remove everything otherwise the cluster will no longer scale-up
                            rm -rf /var/run/etcd/*
                        fi
                    fi
          command:
            - /bin/sh
            - -ec
            - |
              set -x
              HOSTNAME=$(hostname)
              if [ ! -d "/persistence/etcd/${HOSTNAME}" ]; then
                  mkdir -p /persistence/etcd/${HOSTNAME}
              fi
              ln -sf /persistence/etcd/${HOSTNAME} /var/run/etcd
              # store member id into PVC for later member replacement
              collect_member() {
                  while ! etcdctl member list &>/dev/null; do sleep 1; done
                  etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1 > /var/run/etcd/member_id
                  exit 0
              }
              eps() {
                  EPS=""
                  for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                      EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
                  done
                  echo ${EPS}
              }
              member_hash() {
                  etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1
              }
              # re-joining after failure?
              if [ -e /var/run/etcd/default.etcd ]; then
                  echo "Re-joining etcd member"
                  if [ ! -f /var/run/etcd/member_id ]; then
                      # prestop cleanup
                      EPS=""
                      for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                          EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
                      done
                      HOSTNAME=$(hostname)
                      member_hash() {
                          etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1
                      }

                      #Add in member
                      export ETCDCTL_ENDPOINT=$(eps)
                      # member already added?
                      MEMBER_HASH=$(member_hash)
                      if [ -n "${MEMBER_HASH}" ]; then
                          # the member hash exists but for some reason etcd failed
                          # as the datadir has not be created, we can remove the member
                          # and retrieve new hash
                          etcdctl member remove ${MEMBER_HASH}
                          if [ $? -eq 0 ]; then
                            # Remove everything otherwise the cluster will no longer scale-up
                            rm -rf /var/run/etcd/*
                          fi
                      fi
                      echo "Adding new member"
                      etcdctl member add ${HOSTNAME} http://${HOSTNAME}.${SET_NAME}:2380 | grep "^ETCD_" > /var/run/etcd/new_member_envs
                      if [ $? -ne 0 ]; then
                          echo "Exiting"
                          rm -f /var/run/etcd/new_member_envs
                          exit 1
                      fi
                      cat /var/run/etcd/new_member_envs
                      source /var/run/etcd/new_member_envs
                      collect_member &
                      exec etcd --name ${HOSTNAME} \
                          --listen-peer-urls http://0.0.0.0:2380 \
                          --listen-client-urls http://0.0.0.0:2379 \
                          --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                          --data-dir /var/run/etcd/default.etcd \
                          --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                          --initial-cluster ${ETCD_INITIAL_CLUSTER} \
                          --initial-cluster-state ${ETCD_INITIAL_CLUSTER_STATE}
                  else
                  #same logic as before for when using a pvc.. rejoin member
                  member_id=$(cat /var/run/etcd/member_id)
                  # re-join member
                  ETCDCTL_ENDPOINT=$(eps) etcdctl member update ${member_id} http://${HOSTNAME}.${SET_NAME}:2380 | true
                  exec etcd --name ${HOSTNAME} \
                      --listen-peer-urls http://0.0.0.0:2380 \
                      --listen-client-urls http://0.0.0.0:2379\
                      --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                      --data-dir /var/run/etcd/default.etcd
                  fi
              fi
              # etcd-SET_ID
              SET_ID=${HOSTNAME##*[^0-9]}
              # adding a new member to existing cluster (assuming all initial pods are available)
              if [ "${SET_ID}" -ge ${INITIAL_CLUSTER_SIZE} ]; then
                  export ETCDCTL_ENDPOINT=$(eps)
                  # member already added?
                  MEMBER_HASH=$(member_hash)
                  if [ -n "${MEMBER_HASH}" ]; then
                      # the member hash exists but for some reason etcd failed
                      # as the datadir has not be created, we can remove the member
                      # and retrieve new hash
                      etcdctl member remove ${MEMBER_HASH}
                  fi
                  echo "Adding new member"
                  etcdctl member add ${HOSTNAME} http://${HOSTNAME}.${SET_NAME}:2380 | grep "^ETCD_" > /var/run/etcd/new_member_envs
                  if [ $? -ne 0 ]; then
                      echo "Exiting"
                      rm -f /var/run/etcd/new_member_envs
                      exit 1
                  fi
                  cat /var/run/etcd/new_member_envs
                  source /var/run/etcd/new_member_envs
                  collect_member &
                  exec etcd --name ${HOSTNAME} \
                      --listen-peer-urls http://0.0.0.0:2380 \
                      --listen-client-urls http://0.0.0.0:2379 \
                      --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                      --data-dir /var/run/etcd/default.etcd \
                      --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                      --initial-cluster ${ETCD_INITIAL_CLUSTER} \
                      --initial-cluster-state ${ETCD_INITIAL_CLUSTER_STATE}
              fi
              PEERS=""
              for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                  PEERS="${PEERS}${PEERS:+,}${SET_NAME}-${i}=http://${SET_NAME}-${i}.${SET_NAME}:2380"
              done
              collect_member &
              # join member
              exec etcd --name ${HOSTNAME} \
                  --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                  --listen-peer-urls http://0.0.0.0:2380 \
                  --listen-client-urls http://0.0.0.0:2379 \
                  --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                  --initial-cluster-token etcd-cluster-1 \
                  --initial-cluster ${PEERS} \
                  --initial-cluster-state new \
                  --data-dir /var/run/etcd/default.etcd
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-na-ibm-db2warehouse-db2u
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    component: db2wh
    type: engine
    icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
    icpdsupport/app: release-na-ibm-db2warehouse
spec:
  serviceName: release-na-ibm-db2warehouse-db2u-internal
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app: release-na-ibm-db2warehouse
      chart: ibm-db2warehouse
      release: release-name
      heritage: Helm
      component: db2wh
      type: engine
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: ibm-db2warehouse
        release: release-name
        heritage: Helm
        component: db2wh
        type: engine
        api-progress: db2wh-api
        icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:
        productName: Db2 Warehouse
        productID: Community
        productVersion: 11.5.4.0
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      serviceAccount: db2u
      terminationGracePeriodSeconds: 30
      initContainers:
        - name: init-db2
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          env:
            - name: MEMORY_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: release-na-ibm-db2warehouse-db2u
                  resource: limits.memory
          command:
            - /bin/sh
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: true
            runAsNonRoot: false
            runAsUser: 0
            capabilities:
              add:
                - SYS_RESOURCE
                - IPC_OWNER
                - SYS_NICE
              drop:
                - ALL
          args:
            - -cx
            - /tools/pre-install/set_kernel_params.sh
          volumeMounts:
            - mountPath: /host/proc
              name: proc
              readOnly: false
            - mountPath: /host/proc/sys
              name: sys
              readOnly: false
        - name: init-db2-node-cfg
          securityContext:
            privileged: false
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          command:
            - /bin/sh
            - -ec
            - |
              /tools/pre-install/update_nodeslist.sh
              lineNum="$(grep -n $POD_NAME /mnt/blumeta0/nodeslist | head -n 1 | cut -d: -f1)"
              if [ $lineNum -eq 1 ]; then
                  kubectl label pod $POD_NAME name=dashmpp-head-0 -n $POD_NAMESPACE --overwrite
              else
                  lineNum=$[$lineNum-1]
                  kubectl label pod $POD_NAME name=dashmpp-data${lineNum}-0 -n $POD_NAMESPACE --overwrite
              fi
              kubectl wait --for=condition=complete job/release-na-ibm-db2warehouse-db2u-nodes-cfg-job -n default
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          volumeMounts:
            - mountPath: /mnt/blumeta0
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
      containers:
        - name: release-na-ibm-db2warehouse-db2u
          image: icr.io/obs/hdm/db2u/db2u:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 10979
            capabilities:
              add:
                - SYS_RESOURCE
                - IPC_OWNER
                - SYS_NICE
                - CHOWN
                - DAC_OVERRIDE
                - FSETID
                - FOWNER
                - SETGID
                - SETUID
                - SETFCAP
                - SETPCAP
                - NET_BIND_SERVICE
                - SYS_CHROOT
                - KILL
                - AUDIT_WRITE
              drop:
                - ALL
          resources:
            limits:
              memory: 4.3Gi
              cpu: 2
            requests:
              memory: 4.3Gi
              cpu: 2
          tty: true
          env:
            - name: MEMORY_LIMIT
              valueFrom:
                resourceFieldRef:
                  resource: limits.memory
            - name: etcdoperator
              value: "true"
            - name: POD
              valueFrom:
                fieldRef:
                  fieldPath: metadata.labels['name']
            - name: ETCD_ENDPOINTS
              value: http://release-na-ibm-db2warehouse-etcd-0.release-na-ibm-db2warehouse-etcd:2379,http://release-na-ibm-db2warehouse-etcd-1.release-na-ibm-db2warehouse-etcd:2379,http://release-na-ibm-db2warehouse-etcd-2.release-na-ibm-db2warehouse-etcd:2379
            - name: WV_RECOVERY
              value: partial
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /mnt/blumeta0
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
            - mountPath: /mnt/bludata0
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
            - mountPath: /mnt/blumeta0/configmap/db2u
              name: release-na-ibm-db2warehouse-db2u-config-volume
            - mountPath: /mnt/blumeta0/configmap/wv
              name: release-na-ibm-db2warehouse-db2u-wv-config-volume
            - mountPath: /mnt/blumeta0/configmap/hadr
              name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
            - mountPath: /run
              name: runvol
            - name: db2instance-secret
              mountPath: /secrets/db2instancepwd
              readOnly: true
            - mountPath: /db2u/license
              name: release-na-ibm-db2warehouse-db2u-lic-volume
              readOnly: false
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/bash
                  - -cl
                  - |
                    /usr/bin/stop
                    sudo kill -CONT 1
          ports:
            - containerPort: 50000
              protocol: TCP
              name: db2-server
            - containerPort: 50001
              protocol: TCP
              name: db2-ssl-server
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        - name: sys
          hostPath:
            path: /proc/sys
        - name: proc
          hostPath:
            path: /proc
        - name: runvol
          emptyDir:
            medium: Memory
        - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
          persistentVolumeClaim:
            claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - name: release-na-ibm-db2warehouse-db2u-config-volume
          configMap:
            name: release-na-ibm-db2warehouse-db2u-config
            items:
              - key: db2u-var
                path: db2u-var
        - name: release-na-ibm-db2warehouse-db2u-lic-volume
          configMap:
            name: release-na-ibm-db2warehouse-db2u-lic
            items:
              - key: db2u-lic
                path: db2u-lic
        - name: release-na-ibm-db2warehouse-db2u-wv-config-volume
          configMap:
            name: release-na-ibm-db2warehouse-db2u-wv-config
        - name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
          configMap:
            name: release-na-ibm-db2warehouse-db2u-hadr-config
        - name: db2instance-secret
          secret:
            secretName: release-na-ibm-db2warehouse-db2u-instance
            defaultMode: 256
---
kind: Job
apiVersion: batch/v1
metadata:
  name: release-na-ibm-db2warehouse-db2u-engn-update-job
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
    icpdsupport/app: release-na-ibm-db2warehouse
  annotations:
    productName: Db2 Warehouse
    productID: Community
    productVersion: 11.5.4.0
spec:
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: ibm-db2warehouse
        release: release-name
        heritage: Helm
        icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:
        productName: Db2 Warehouse
        productID: Community
        productVersion: 11.5.4.0
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      securityContext:
        runAsNonRoot: true
      initContainers:
        - name: condition-ready
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          command:
            - /bin/sh
          args:
            - -cx
            - /tools/post-install/db2u_ready.sh --replicas 1 --template release-na-ibm-db2warehouse --namespace default --dbType db2wh
        - name: detect-vrmf-change
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          command:
            - /bin/sh
            - -ec
            - |
              DETERMINATION_FILE=/mnt/blumeta0/nodeslist
              CAT_NODE=$(head -1 $DETERMINATION_FILE)
              # After INSTDB job completes, Db2 instance home is persisted on disk. Which is a
              # prerequisite for the VRMF detection code, since it depends on ~/sqllib/.instuse file.
              kubectl wait --for=condition=complete job/release-na-ibm-db2warehouse-db2u-sqllib-shared-job -n default
              kubectl exec -it -n default ${CAT_NODE?} -- bash -c "sudo /db2u/scripts/detect_db2_vrmf_change.sh -file"
          volumeMounts:
            - mountPath: /mnt/blumeta0
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
      containers:
        - name: engn-update
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          command:
            - /bin/sh
            - -c
            - "DETERMINATION_FILE=/mnt/blumeta0/nodeslist\nCAT_NODE=$(head -1 $DETERMINATION_FILE)\ncmd=\"\"\nupdt_upgrd_opt=\"-all\"\nhadr_enabled=\"false\"\nRC=0\n\nkubectl exec -it -n default ${CAT_NODE?} -- bash -c '[[ -f /mnt/blumeta0/vrmf/change.out ]] || exit 0; exit $(cat /mnt/blumeta0/vrmf/change.out)' 2>/dev/null\nvrmf_chk=$?\necho \"VRMF check status bit: ${vrmf_chk}\"\n\n# If HADR is enabled dont run the DB update/upgrade scripts. This will be handled\n# by external mechanics to work around rolling updates.\nkubectl exec -it -n default ${CAT_NODE?} -- bash -c 'grep -qE \"^HADR_ENABLED.*true\" /mnt/blumeta0/configmap/hadr/*' 2>/dev/null\n[[ $? -eq 0 ]] && hadr_enabled=\"true\"\n[[ \"${hadr_enabled}\" == \"true\" ]] && updt_upgrd_opt=\"-inst\"\n\n# Check VRMF change bit and execute Db2 update or upgrade process\nif [[ $vrmf_chk -ne 0 ]]; then\n    if [[ $vrmf_chk -eq 1 ]]; then\n        echo \"Running the Db2 engine update script ...\"\n        cmd=\"su - db2inst1 -c '/db2u/scripts/db2u_update.sh ${updt_upgrd_opt}'\"\n    elif [[ $vrmf_chk -eq 2 ]]; then\n        echo \"Running the Db2 engine upgrade script ...\"\n        cmd=\"su - db2inst1 -c '/db2u/scripts/db2u_upgrade.sh ${updt_upgrd_opt}'\"\n    fi\n    [[ -n \"$cmd\" ]] && kubectl exec -it -n default ${CAT_NODE?} -- bash -c \"$cmd\"\n    RC=$?\n    [[ $RC -ne 0 ]] && exit $RC\n\n    # If HADR is enabled, dont start Woliverine HA\n    [[ \"${hadr_enabled}\" == \"true\" ]] && exit $RC\n\n    # For all other Db2 engine update/upgrade scenarios, start Woliverine HA on all Db2U PODs now\n    echo \"Starting Wolverine HA ...\"\n    cmd=\"source /db2u/scripts/include/common_functions.sh && start_wvha_allnodes\"\n    kubectl exec -it -n default ${CAT_NODE?} -- bash -c \"$cmd\"\n    RC=$?\nfi\nexit $RC        \n"
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          volumeMounts:
            - mountPath: /mnt/blumeta0
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
            - mountPath: /mnt/blumeta0/configmap/hadr
              name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
      restartPolicy: Never
      serviceAccount: db2u
      volumes:
        - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
          persistentVolumeClaim:
            claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
          configMap:
            name: release-na-ibm-db2warehouse-db2u-hadr-config
---
kind: Job
apiVersion: batch/v1
metadata:
  name: release-na-ibm-db2warehouse-db2u-nodes-cfg-job
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
    icpdsupport/app: release-na-ibm-db2warehouse
  annotations:
    productName: Db2 Warehouse
    productID: Community
    productVersion: 11.5.4.0
spec:
  backoffLimit: 30
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: ibm-db2warehouse
        release: release-name
        heritage: Helm
        icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:
        productName: Db2 Warehouse
        productID: Community
        productVersion: 11.5.4.0
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      initContainers:
        - name: db2u-sqllib-shared-job
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          command:
            - /bin/sh
          args:
            - -cx
            - kubectl wait --for=condition=complete job/release-na-ibm-db2warehouse-db2u-sqllib-shared-job -n default
      containers:
        - name: db2u-db2nodes-cfg-init
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          command:
            - /bin/sh
          args:
            - -cx
            - /tools/pre-install/generate_mln_distribution.sh -mlns 1 -replicas 1
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          volumeMounts:
            - mountPath: /mnt/blumeta0
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
            - mountPath: /mnt/blumeta0/configmap/db2u
              name: release-na-ibm-db2warehouse-db2u-config-volume
      restartPolicy: Never
      serviceAccount: db2u
      volumes:
        - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
          persistentVolumeClaim:
            claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - name: release-na-ibm-db2warehouse-db2u-config-volume
          configMap:
            name: release-na-ibm-db2warehouse-db2u-config
            items:
              - key: db2u-var
                path: db2u-var
---
kind: Job
apiVersion: batch/v1
metadata:
  name: release-na-ibm-db2warehouse-db2u-restore-morph-job
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
    icpdsupport/app: release-na-ibm-db2warehouse
  annotations:
    productName: Db2 Warehouse
    productID: Community
    productVersion: 11.5.4.0
spec:
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: ibm-db2warehouse
        release: release-name
        heritage: Helm
        icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:
        productName: Db2 Warehouse
        productID: Community
        productVersion: 11.5.4.0
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      securityContext:
        runAsNonRoot: true
      initContainers:
        - name: condition-ready
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          command:
            - /bin/sh
          args:
            - -cx
            - /tools/post-install/db2u_ready.sh --replicas 1 --template release-na-ibm-db2warehouse --namespace default --dbType db2wh
      containers:
        - name: restore-morph
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          command:
            - /bin/sh
            - -ec
            - "DETERMINATION_FILE=/mnt/blumeta0/nodeslist\nCAT_NODE=$(head -1 $DETERMINATION_FILE)\nkubectl exec -it -n default ${CAT_NODE?} -- bash -c \"su - db2inst1 -c \\\"/db2u/db2u_restore_morph.sh\\\"\"\nexit $?        \n"
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          volumeMounts:
            - mountPath: /mnt/blumeta0
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
            - mountPath: /mnt/blumeta0/configmap/db2u
              name: release-na-ibm-db2warehouse-db2u-config-volume
      restartPolicy: Never
      serviceAccount: db2u
      volumes:
        - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
          persistentVolumeClaim:
            claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - name: release-na-ibm-db2warehouse-db2u-config-volume
          configMap:
            name: release-na-ibm-db2warehouse-db2u-config
            items:
              - key: db2u-var
                path: db2u-var
---
kind: Job
apiVersion: batch/v1
metadata:
  name: release-na-ibm-db2warehouse-db2u-sqllib-shared-job
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
    icpdsupport/app: release-na-ibm-db2warehouse
  annotations:
    productName: Db2 Warehouse
    productID: Community
    productVersion: 11.5.4.0
spec:
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: ibm-db2warehouse
        release: release-name
        heritage: Helm
        icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:
        productName: Db2 Warehouse
        productID: Community
        productVersion: 11.5.4.0
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      securityContext:
        runAsNonRoot: true
      containers:
        - name: release-na-ibm-db2warehouse-db2-init
          image: icr.io/obs/hdm/db2u/db2u.instdb:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 10777
            capabilities:
              add:
                - FOWNER
                - SETGID
                - SETUID
                - CHOWN
                - DAC_OVERRIDE
              drop:
                - ALL
          command:
            - /bin/sh
          args:
            - -cx
            - /Db2wh_preinit/instdb_entrypoint.sh
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 200Mi
          env:
            - name: SERVICENAME
              value: ""
          volumeMounts:
            - mountPath: /mnt/blumeta0
              name: release-na-ibm-db2warehouse-db2u-sqllib-shared
            - mountPath: /mnt/blumeta0/configmap/db2u
              name: release-na-ibm-db2warehouse-db2u-config-volume
      restartPolicy: Never
      serviceAccount: db2u
      volumes:
        - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
          persistentVolumeClaim:
            claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - name: release-na-ibm-db2warehouse-db2u-config-volume
          configMap:
            name: release-na-ibm-db2warehouse-db2u-config
            items:
              - key: db2u-var
                path: db2u-var
---
apiVersion: v1
kind: Pod
metadata:
  name: test-connection
  annotations:
    helm.sh/hook: test-success
  labels:
    app: release-na-ibm-db2warehouse-test
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    component: db2wh
spec:
  restartPolicy: Never
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                  - amd64
  hostNetwork: false
  hostPID: false
  hostIPC: false
  securityContext:
    runAsNonRoot: true
  containers:
    - name: release-na-ibm-db2warehouse-db2u-test
      image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
      securityContext:
        privileged: false
        readOnlyRootFilesystem: false
        allowPrivilegeEscalation: false
        runAsNonRoot: true
        runAsUser: 500
        capabilities:
          drop:
            - ALL
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
        limits:
          cpu: 200m
          memory: 500Mi
      command:
        - /bin/sh
      args:
        - -cx
        - |
          pods=$(kubectl get pods --selector=app=release-na-ibm-db2warehouse --selector=type=engine --no-headers | awk '{print($1)}')
          for i in ${pods//|/ }; do
                kubectl exec -ti ${i} -- su - db2inst1 -c "db2 connect to bludb"
                rc=$?
                if [[ ${rc} -ne 0 ]]; then
                    exit ${rc}
                fi
          done
          exit $?
---
kind: Job
apiVersion: batch/v1
metadata:
  name: release-na-ibm-db2warehouse-db2u-preupgrade-hook-v3
  labels:
    app: release-na-ibm-db2warehouse
    chart: ibm-db2warehouse
    release: release-name
    heritage: Helm
    icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
    icpdsupport/app: release-na-ibm-db2warehouse
  annotations:
    helm.sh/hook: pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    productName: Db2 Warehouse
    productID: Community
    productVersion: 11.5.4.0
spec:
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: ibm-db2warehouse
        release: release-name
        heritage: Helm
        icpdsupport/serviceInstanceId: release-na-ibm-db2warehouse
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:
        productName: Db2 Warehouse
        productID: Community
        productVersion: 11.5.4.0
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      securityContext:
        runAsNonRoot: true
      containers:
        - name: stop-slapd
          image: icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: false
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 500
            capabilities:
              drop:
                - ALL
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
            limits:
              cpu: 200m
              memory: 100Mi
          command:
            - /bin/sh
            - -ecx
            - |
              ldap_pods=$(kubectl get -n default po -l component=ldap,release=release-name -o jsonpath='{.items[*].metadata.name}')
              for pod in ${ldap_pods}; do
                    kubectl exec -it -n default ${pod} -- bash -c "which supervisorctl && supervisorctl stop slapd || exit 0"
              done
      restartPolicy: Never
      serviceAccount: db2u
